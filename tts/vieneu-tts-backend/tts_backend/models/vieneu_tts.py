"""
VieNeu-TTS Model Wrapper
Wrapper cho Model VieNeu-TTS

This wrapper uses the SAME environment as VieNeu-TTS for 100% compatibility.
Wrapper nÃ y sá»­ dá»¥ng CÃ™NG mÃ´i trÆ°á»ng vá»›i VieNeu-TTS Ä‘á»ƒ Ä‘áº£m báº£o 100% tÆ°Æ¡ng thÃ­ch.

NO PATCHES NEEDED - We're using VieNeu-TTS's working environment!
KHÃ”NG Cáº¦N PATCH - ChÃºng ta Ä‘ang sá»­ dá»¥ng mÃ´i trÆ°á»ng hoáº¡t Ä‘á»™ng cá»§a VieNeu-TTS!
"""
import sys
import warnings
import os
from pathlib import Path
from typing import Optional
import torch
import soundfile as sf
import numpy as np

# Suppress warnings EXACTLY like test_female_voice.py does
# Táº¯t cáº£nh bÃ¡o CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
warnings.filterwarnings('ignore')
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# Add VieNeu-TTS repo to path FIRST (before any imports)
# ThÃªm repo VieNeu-TTS vÃ o path TRÆ¯á»šC (trÆ°á»›c má»i import)
# This is the SAME setup as test_female_voice.py that works!
# ÄÃ¢y lÃ  setup GIá»NG NHÆ¯ test_female_voice.py Ä‘Ã£ hoáº¡t Ä‘á»™ng!
# File structure: tts/vieneu-tts-backend/tts_backend/models/vieneu_tts.py
# Go up 5 levels to project root: models -> tts_backend -> vieneu-tts-backend -> tts -> novel-reader
# Then: project_root/tts/VieNeu-TTS
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
VIENEU_REPO_PATH = PROJECT_ROOT / "tts" / "VieNeu-TTS"

if not VIENEU_REPO_PATH.exists():
    raise ImportError(
        f"VieNeu-TTS repository not found at: {VIENEU_REPO_PATH}\n"
        f"Repository VieNeu-TTS khÃ´ng tÃ¬m tháº¥y táº¡i: {VIENEU_REPO_PATH}\n"
        f"Expected location: tts/VieNeu-TTS relative to project root: {PROJECT_ROOT}"
    )

if str(VIENEU_REPO_PATH) not in sys.path:
    sys.path.insert(0, str(VIENEU_REPO_PATH))
    print(f"âœ… Added VieNeu-TTS repo to path: {VIENEU_REPO_PATH}")
    print(f"âœ… ÄÃ£ thÃªm repo VieNeu-TTS vÃ o path: {VIENEU_REPO_PATH}")

# Import EXACTLY like test_female_voice.py does (working example)
# Import CHÃNH XÃC nhÆ° test_female_voice.py lÃ m (vÃ­ dá»¥ hoáº¡t Ä‘á»™ng)
# No patches needed - we're using the same environment!
# KhÃ´ng cáº§n patch - chÃºng ta Ä‘ang sá»­ dá»¥ng cÃ¹ng mÃ´i trÆ°á»ng!
from vieneu_tts import VieNeuTTS

# Try to import config_local like the working test does
# Thá»­ import config_local nhÆ° test hoáº¡t Ä‘á»™ng lÃ m
try:
    from config_local import get_backbone_repo
    USE_LOCAL_CONFIG = True
except ImportError:
    USE_LOCAL_CONFIG = False
    # Fallback to our config system
    from config import ModelConfig


class VieNeuTTSWrapper:
    """
    Wrapper for VieNeu-TTS model / Wrapper cho model VieNeu-TTS
    
    This follows the exact initialization pattern from VieNeu-TTS repository examples.
    Class nÃ y tuÃ¢n theo Ä‘Ãºng pattern khá»Ÿi táº¡o tá»« cÃ¡c vÃ­ dá»¥ trong repository VieNeu-TTS.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize VieNeu-TTS model / Khá»Ÿi táº¡o model VieNeu-TTS
        
        Args:
            model_path: Path to local model directory / ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c model local
                       If None, uses default from config
                       Náº¿u None, sá»­ dá»¥ng máº·c Ä‘á»‹nh tá»« config
            device: Device to use (cuda/cpu/auto) / Thiáº¿t bá»‹ sá»­ dá»¥ng
                   If None, auto-detects (cuda if available, else cpu)
                   Náº¿u None, tá»± Ä‘á»™ng phÃ¡t hiá»‡n (cuda náº¿u cÃ³, khÃ´ng thÃ¬ cpu)
        """
        # Get model path EXACTLY like test_female_voice.py does
        # Láº¥y Ä‘Æ°á»ng dáº«n model CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
        if model_path:
            self.model_path = model_path
        elif USE_LOCAL_CONFIG:
            # Use config_local.get_backbone_repo() like the working test
            # Sá»­ dá»¥ng config_local.get_backbone_repo() nhÆ° test hoáº¡t Ä‘á»™ng
            self.model_path = get_backbone_repo()
        else:
            # Fallback to our config system
            self.model_path = ModelConfig.VIENEU_TTS["model_path"]
        
        # Determine device EXACTLY like test_female_voice.py does
        # XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹ CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # Sample rate is always 24kHz for VieNeu-TTS
        # Táº§n sá»‘ láº¥y máº«u luÃ´n lÃ  24kHz cho VieNeu-TTS
        self.sample_rate = 24_000
        
        # Initialize model EXACTLY like test_female_voice.py does (working example)
        # Khá»Ÿi táº¡o model CHÃNH XÃC nhÆ° test_female_voice.py lÃ m (vÃ­ dá»¥ hoáº¡t Ä‘á»™ng)
        print(f"ðŸ–¥ï¸  Using device: {self.device}")
        print(f"ðŸ–¥ï¸  Sá»­ dá»¥ng thiáº¿t bá»‹: {self.device}")
        print(f"ðŸ“¦ Loading model from: {self.model_path}")
        print(f"ðŸ“¦ Äang táº£i model tá»«: {self.model_path}")
        
        # Initialize EXACTLY like test_female_voice.py: VieNeuTTS(...)
        # Khá»Ÿi táº¡o CHÃNH XÃC nhÆ° test_female_voice.py: VieNeuTTS(...)
        self.model = VieNeuTTS(
            backbone_repo=self.model_path,
            backbone_device=self.device,
            codec_repo="neuphonic/neucodec",
            codec_device=self.device
        )
        
        # Apply performance optimizations for GPU (similar to Dia TTS)
        # Ãp dá»¥ng tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cho GPU (tÆ°Æ¡ng tá»± Dia TTS)
        if self.device == "cuda":
            self._setup_cuda_optimizations()
            self._apply_model_optimizations()
        
        print("âœ… VieNeu-TTS loaded successfully")
        print("âœ… VieNeu-TTS Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng")
        
        # Model is now loaded to GPU, optimizations applied
        # Model Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn GPU, cÃ¡c tá»‘i Æ°u hÃ³a Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng
    
    def warmup(self, ref_audio_path: Optional[str] = None, ref_text: Optional[str] = None):
        """
        Warmup model with a dummy inference to prepare GPU for fast inference.
        LÃ m nÃ³ng model vá»›i inference giáº£ Ä‘á»ƒ chuáº©n bá»‹ GPU cho inference nhanh.
        
        This should be called after model is loaded to ensure fast first inference.
        NÃªn Ä‘Æ°á»£c gá»i sau khi model Ä‘Æ°á»£c táº£i Ä‘á»ƒ Ä‘áº£m báº£o inference Ä‘áº§u tiÃªn nhanh.
        
        Note: torch.compile is disabled for VieNeu-TTS due to Qwen2 architecture
        incompatibility with scaled_dot_product_attention. Other optimizations (TF32,
        FP16, Flash Attention) are still active.
        LÆ°u Ã½: torch.compile bá»‹ táº¯t cho VieNeu-TTS do khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i kiáº¿n trÃºc
        Qwen2 vÃ  scaled_dot_product_attention. CÃ¡c tá»‘i Æ°u hÃ³a khÃ¡c (TF32, FP16,
        Flash Attention) váº«n hoáº¡t Ä‘á»™ng.
        
        Args:
            ref_audio_path: Optional reference audio path for warmup / ÄÆ°á»ng dáº«n audio tham chiáº¿u tÃ¹y chá»n Ä‘á»ƒ warmup
            ref_text: Optional reference text for warmup / VÄƒn báº£n tham chiáº¿u tÃ¹y chá»n Ä‘á»ƒ warmup
        """
        if self.device != "cuda":
            # No need to warmup on CPU
            print("â„¹ï¸  Skipping warmup (CPU mode)")
            print("â„¹ï¸  Bá» qua warmup (cháº¿ Ä‘á»™ CPU)")
            return
        
        print("ðŸ”¥ Warming up model (preparing GPU for fast inference)...")
        print("ðŸ”¥ Äang lÃ m nÃ³ng model (chuáº©n bá»‹ GPU cho inference nhanh)...")
        
        try:
            # Use default voice if not provided
            # Sá»­ dá»¥ng giá»ng máº·c Ä‘á»‹nh náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
            if not ref_audio_path or not ref_text:
                # Get default voice from voice selector
                # Láº¥y giá»ng máº·c Ä‘á»‹nh tá»« voice selector
                from ..voice_selector import VOICE_SAMPLES, get_sample_path
                default_voice_id = "id_0004"  # Female voice default
                voice_info = VOICE_SAMPLES.get(default_voice_id, list(VOICE_SAMPLES.values())[0])
                
                # Get paths relative to VieNeu-TTS repo
                # Láº¥y Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i vá»›i repo VieNeu-TTS
                sample_dir = VIENEU_REPO_PATH / "sample"
                ref_audio_path = str(sample_dir / voice_info["audio"])
                ref_text_path = sample_dir / voice_info["text"]
                
                if ref_text_path.exists():
                    with open(ref_text_path, "r", encoding="utf-8") as f:
                        ref_text = f.read().strip()
                else:
                    ref_text = "Xin chÃ o."  # Fallback dummy text
            
            # Short dummy text for warmup
            # VÄƒn báº£n giáº£ ngáº¯n cho warmup
            dummy_text = "Xin chÃ o."
            
            # Perform a dummy inference to warmup the model and GPU
            # Thá»±c hiá»‡n inference giáº£ Ä‘á»ƒ lÃ m nÃ³ng model vÃ  GPU
            print("   Running warmup inference (this may take 30-60 seconds)...")
            print("   Äang cháº¡y warmup inference (cÃ³ thá»ƒ máº¥t 30-60 giÃ¢y)...")
            
            # First inference: Warms up the model and prepares GPU kernels
            # Inference Ä‘áº§u tiÃªn: LÃ m nÃ³ng model vÃ  chuáº©n bá»‹ GPU kernels
            _ = self.model.infer(dummy_text, self.model.encode_reference(ref_audio_path), ref_text)
            
            # Skip torch.compile for VieNeu-TTS - not compatible with scaled_dot_product_attention
            # Bá» qua torch.compile cho VieNeu-TTS - khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i scaled_dot_product_attention
            # The model uses Qwen2 architecture which has issues with torch.compile
            # Model sá»­ dá»¥ng kiáº¿n trÃºc Qwen2 cÃ³ váº¥n Ä‘á» vá»›i torch.compile
            print("   â„¹ï¸  torch.compile disabled for VieNeu-TTS (incompatible with attention mechanism)")
            print("   â„¹ï¸  torch.compile Ä‘Ã£ bá»‹ táº¯t cho VieNeu-TTS (khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i cÆ¡ cháº¿ attention)")
            print("   â„¹ï¸  Using TF32, FP16, and Flash Attention optimizations instead")
            print("   â„¹ï¸  Sá»­ dá»¥ng cÃ¡c tá»‘i Æ°u hÃ³a TF32, FP16, vÃ  Flash Attention thay tháº¿")
            self._torch_compile_enabled = False
            
            print("âœ… Model warmup completed!")
            print("âœ… Model warmup hoÃ n táº¥t!")
            print("   Model is now optimized and ready for fast inference!")
            print("   Model Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u vÃ  sáºµn sÃ ng cho inference nhanh!")
        except Exception as e:
            print(f"âš ï¸  Warmup failed (non-critical): {e}")
            print(f"âš ï¸  Warmup tháº¥t báº¡i (khÃ´ng nghiÃªm trá»ng): {e}")
            print("   Model will still work, but first inference may be slower")
            print("   Model váº«n sáº½ hoáº¡t Ä‘á»™ng, nhÆ°ng inference Ä‘áº§u tiÃªn cÃ³ thá»ƒ cháº­m hÆ¡n")
    
    def _setup_cuda_optimizations(self):
        """
        Setup CUDA optimizations (TF32, Flash Attention) for better performance.
        Thiáº¿t láº­p tá»‘i Æ°u hÃ³a CUDA (TF32, Flash Attention) Ä‘á»ƒ hiá»‡u suáº¥t tá»‘t hÆ¡n.
        """
        try:
            # Enable TF32 for Ampere+ GPUs (RTX 4090 supports this)
            # Báº­t TF32 cho GPU Ampere+ (RTX 4090 há»— trá»£)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            print("ðŸš€ CUDA optimizations enabled:")
            print("ðŸš€ Tá»‘i Æ°u hÃ³a CUDA Ä‘Ã£ Ä‘Æ°á»£c báº­t:")
            print("   - TF32 enabled for faster matmul operations")
            print("   - TF32 Ä‘Ã£ Ä‘Æ°á»£c báº­t cho cÃ¡c phÃ©p toÃ¡n matmul nhanh hÆ¡n")
            
            # Flash Attention disabled - causes "No available kernel" error with Qwen2
            # Flash Attention bá»‹ táº¯t - gÃ¢y ra lá»—i "No available kernel" vá»›i Qwen2
            self._flash_attention_available = False
            print("   - Flash Attention disabled (incompatible with Qwen2 attention mechanism)")
            print("   - Flash Attention Ä‘Ã£ bá»‹ táº¯t (khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i cÆ¡ cháº¿ attention cá»§a Qwen2)")
            print("   - TF32 and FP16 optimizations still active")
            print("   - CÃ¡c tá»‘i Æ°u hÃ³a TF32 vÃ  FP16 váº«n hoáº¡t Ä‘á»™ng")
        except Exception as e:
            print(f"âš ï¸  Warning: Could not enable all CUDA optimizations: {e}")
            print(f"âš ï¸  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ báº­t táº¥t cáº£ tá»‘i Æ°u hÃ³a CUDA: {e}")
    
    def _apply_model_optimizations(self):
        """
        Apply model-level optimizations (half precision).
        Ãp dá»¥ng tá»‘i Æ°u hÃ³a cáº¥p Ä‘á»™ model (half precision).
        
        Note: torch.compile is disabled for VieNeu-TTS due to Qwen2 architecture incompatibility
        with scaled_dot_product_attention. Other optimizations (TF32, FP16, Flash Attention) are still active.
        LÆ°u Ã½: torch.compile bá»‹ táº¯t cho VieNeu-TTS do khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i kiáº¿n trÃºc Qwen2
        vÃ  scaled_dot_product_attention. CÃ¡c tá»‘i Æ°u hÃ³a khÃ¡c (TF32, FP16, Flash Attention) váº«n hoáº¡t Ä‘á»™ng.
        """
        try:
            # Enable half precision (fp16) for faster inference and less memory
            # Báº­t half precision (fp16) Ä‘á»ƒ inference nhanh hÆ¡n vÃ  Ã­t bá»™ nhá»› hÆ¡n
            try:
                if hasattr(self.model, 'backbone'):
                    # Use autocast for fp16 during inference (safer than model.half())
                    # Sá»­ dá»¥ng autocast cho fp16 trong lÃºc inference (an toÃ n hÆ¡n model.half())
                    self._use_fp16 = True
                    print("   âœ… FP16 (half precision) will be used during inference")
                    print("   âœ… FP16 (half precision) sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng trong lÃºc inference")
            except Exception as e:
                print(f"   âš ï¸  FP16 optimization failed: {e}")
                print(f"   âš ï¸  Tá»‘i Æ°u hÃ³a FP16 tháº¥t báº¡i: {e}")
                self._use_fp16 = False
            
            # torch.compile is disabled for VieNeu-TTS (Qwen2 architecture incompatibility)
            # torch.compile bá»‹ táº¯t cho VieNeu-TTS (khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i kiáº¿n trÃºc Qwen2)
            self._torch_compile_enabled = False
            print("   â„¹ï¸  torch.compile disabled (VieNeu-TTS uses Qwen2 - incompatible)")
            print("   â„¹ï¸  torch.compile Ä‘Ã£ bá»‹ táº¯t (VieNeu-TTS dÃ¹ng Qwen2 - khÃ´ng tÆ°Æ¡ng thÃ­ch)")
        except Exception as e:
            print(f"âš ï¸  Warning: Model optimizations failed: {e}")
            print(f"âš ï¸  Cáº£nh bÃ¡o: Tá»‘i Æ°u hÃ³a model tháº¥t báº¡i: {e}")
    
    def synthesize(
        self,
        text: str,
        ref_audio_path: str,
        ref_text: str,
        output_path: Optional[str] = None,
        max_chars: int = 256,
        auto_chunk: bool = True
    ) -> np.ndarray:
        """
        Synthesize speech / Tá»•ng há»£p giá»ng nÃ³i
        
        Supports long text generation by chunking (like infer_long_text.py).
        Há»— trá»£ táº¡o vÄƒn báº£n dÃ i báº±ng cÃ¡ch chia nhá» (nhÆ° infer_long_text.py).
        
        This follows the exact pattern from VieNeu-TTS repository examples.
        Function nÃ y tuÃ¢n theo Ä‘Ãºng pattern tá»« cÃ¡c vÃ­ dá»¥ trong repository VieNeu-TTS.
        
        Args:
            text: Input text / VÄƒn báº£n Ä‘áº§u vÃ o
            ref_audio_path: Path to reference audio / ÄÆ°á»ng dáº«n audio tham chiáº¿u
            ref_text: Reference text (must match the reference audio) / VÄƒn báº£n tham chiáº¿u (pháº£i khá»›p vá»›i audio tham chiáº¿u)
            output_path: Optional output path / ÄÆ°á»ng dáº«n Ä‘áº§u ra tÃ¹y chá»n
            max_chars: Maximum characters per chunk (default: 256) / KÃ½ tá»± tá»‘i Ä‘a má»—i chunk (máº·c Ä‘á»‹nh: 256)
            auto_chunk: Automatically chunk long text (default: True) / Tá»± Ä‘á»™ng chia nhá» vÄƒn báº£n dÃ i (máº·c Ä‘á»‹nh: True)
            
        Returns:
            Audio array (numpy array) / Máº£ng audio (numpy array)
        """
        # Import chunking utility / Import tiá»‡n Ã­ch chia nhá»
        import sys
        from pathlib import Path
        chunker_path = Path(__file__).parent.parent / "text_chunker.py"
        if str(chunker_path.parent) not in sys.path:
            sys.path.insert(0, str(chunker_path.parent))
        from text_chunker import split_text_into_chunks, should_chunk_text
        
        # Encode reference audio ONCE (reused for all chunks) / MÃ£ hÃ³a audio tham chiáº¿u Má»˜T Láº¦N (tÃ¡i sá»­ dá»¥ng cho táº¥t cáº£ chunks)
        ref_codes = self.model.encode_reference(ref_audio_path)
        
        # Check if text needs chunking / Kiá»ƒm tra xem vÄƒn báº£n cÃ³ cáº§n chia nhá» khÃ´ng
        if auto_chunk and should_chunk_text(text, max_chars):
            # Split into chunks / Chia thÃ nh chunks
            chunks = split_text_into_chunks(text, max_chars=max_chars)
            
            if not chunks:
                raise ValueError("Text could not be segmented into valid chunks")
            
            print(f"ðŸ“„ Long text detected: splitting into {len(chunks)} chunks (â‰¤{max_chars} chars each)")
            print(f"ðŸ“„ PhÃ¡t hiá»‡n vÄƒn báº£n dÃ i: chia thÃ nh {len(chunks)} chunks (â‰¤{max_chars} kÃ½ tá»± má»—i chunk)")
            
            # Generate audio for each chunk / Táº¡o audio cho má»—i chunk
            generated_segments = []
            for idx, chunk in enumerate(chunks, start=1):
                print(f"ðŸŽ™ï¸ Generating chunk {idx}/{len(chunks)} ({len(chunk)} chars) / Äang táº¡o chunk {idx}/{len(chunks)} ({len(chunk)} kÃ½ tá»±)")
                # Reuse same ref_codes for all chunks (key optimization!) / TÃ¡i sá»­ dá»¥ng cÃ¹ng ref_codes cho táº¥t cáº£ chunks (tá»‘i Æ°u quan trá»ng!)
                # Use optimized inference with fp16 if available / Sá»­ dá»¥ng inference tá»‘i Æ°u vá»›i fp16 náº¿u cÃ³
                wav = self._infer_optimized(chunk, ref_codes, ref_text)
                generated_segments.append(wav)
            
            # Concatenate all segments / Ná»‘i táº¥t cáº£ cÃ¡c Ä‘oáº¡n
            combined_audio = np.concatenate(generated_segments)
            
            # Save if output path provided / LÆ°u náº¿u cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº§u ra
            if output_path:
                sf.write(output_path, combined_audio, self.sample_rate)
            
            print(f"âœ… Generated long text audio ({len(chunks)} chunks combined) / ÄÃ£ táº¡o audio vÄƒn báº£n dÃ i ({len(chunks)} chunks Ä‘Ã£ káº¿t há»£p)")
            return combined_audio
        else:
            # Short text - generate directly / VÄƒn báº£n ngáº¯n - táº¡o trá»±c tiáº¿p
            # Use optimized inference with fp16 if available / Sá»­ dá»¥ng inference tá»‘i Æ°u vá»›i fp16 náº¿u cÃ³
            wav = self._infer_optimized(text, ref_codes, ref_text)
            
            # Save if output path provided / LÆ°u náº¿u cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº§u ra
            if output_path:
                sf.write(output_path, wav, self.sample_rate)
            
            return wav
    
    def _infer_optimized(self, text: str, ref_codes, ref_text: str) -> np.ndarray:
        """
        Optimized inference with fp16 support for faster generation.
        Inference tá»‘i Æ°u vá»›i há»— trá»£ fp16 Ä‘á»ƒ táº¡o nhanh hÆ¡n.
        
        Note: Flash Attention is disabled due to compatibility issues with Qwen2.
        LÆ°u Ã½: Flash Attention bá»‹ táº¯t do váº¥n Ä‘á» tÆ°Æ¡ng thÃ­ch vá»›i Qwen2.
        """
        # For now, use standard inference to avoid "No available kernel" errors
        # Táº¡m thá»i, dÃ¹ng inference tiÃªu chuáº©n Ä‘á»ƒ trÃ¡nh lá»—i "No available kernel"
        # The "No available kernel" error comes from Flash Attention trying to use
        # kernels that don't exist or aren't compatible with Qwen2's attention mechanism
        # Lá»—i "No available kernel" Ä‘áº¿n tá»« Flash Attention cá»‘ sá»­ dá»¥ng kernels
        # khÃ´ng tá»“n táº¡i hoáº·c khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i cÆ¡ cháº¿ attention cá»§a Qwen2
        
        # Standard inference is still fast with TF32 enabled
        # Inference tiÃªu chuáº©n váº«n nhanh vá»›i TF32 Ä‘Ã£ Ä‘Æ°á»£c báº­t
        return self.model.infer(text, ref_codes, ref_text)
        
        # TODO: Re-enable optimized inference once Flash Attention compatibility is fixed
        # TODO: Báº­t láº¡i inference tá»‘i Æ°u khi tÆ°Æ¡ng thÃ­ch Flash Attention Ä‘Æ°á»£c sá»­a
        # The optimized path below causes "No available kernel" errors:
        # ÄÆ°á»ng dáº«n tá»‘i Æ°u dÆ°á»›i Ä‘Ã¢y gÃ¢y ra lá»—i "No available kernel":
        #
        # if hasattr(self, '_use_fp16') and self._use_fp16 and self.device == "cuda":
        #     # Use fp16 with autocast (safer than forcing Flash Attention)
        #     # Sá»­ dá»¥ng fp16 vá»›i autocast (an toÃ n hÆ¡n Ã©p Flash Attention)
        #     with torch.cuda.amp.autocast(dtype=torch.float16):
        #         # Use standard model.infer() - it handles attention internally
        #         # Sá»­ dá»¥ng model.infer() tiÃªu chuáº©n - nÃ³ xá»­ lÃ½ attention ná»™i bá»™
        #         return self.model.infer(text, ref_codes, ref_text)
    
    def get_sample_rate(self) -> int:
        """Get sample rate / Láº¥y táº§n sá»‘ láº¥y máº«u"""
        return self.sample_rate

