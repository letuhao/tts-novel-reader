"""
VieNeu-TTS Model Wrapper
Wrapper cho Model VieNeu-TTS

This wrapper uses the SAME environment as VieNeu-TTS for 100% compatibility.
Wrapper nÃ y sá»­ dá»¥ng CÃ™NG mÃ´i trÆ°á»ng vá»›i VieNeu-TTS Ä‘á»ƒ Ä‘áº£m báº£o 100% tÆ°Æ¡ng thÃ­ch.

NO PATCHES NEEDED - We're using VieNeu-TTS's working environment!
KHÃ”NG Cáº¦N PATCH - ChÃºng ta Ä‘ang sá»­ dá»¥ng mÃ´i trÆ°á»ng hoáº¡t Ä‘á»™ng cá»§a VieNeu-TTS!
"""
import sys
import warnings
import os
from pathlib import Path
from typing import Optional
import torch
import soundfile as sf
import numpy as np

# Suppress warnings EXACTLY like test_female_voice.py does
# Táº¯t cáº£nh bÃ¡o CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
warnings.filterwarnings('ignore')
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# Add VieNeu-TTS repo to path FIRST (before any imports)
# ThÃªm repo VieNeu-TTS vÃ o path TRÆ¯á»šC (trÆ°á»›c má»i import)
# This is the SAME setup as test_female_voice.py that works!
# ÄÃ¢y lÃ  setup GIá»NG NHÆ¯ test_female_voice.py Ä‘Ã£ hoáº¡t Ä‘á»™ng!
# File structure: tts/vieneu-tts-backend/tts_backend/models/vieneu_tts.py
# Go up 5 levels to project root: models -> tts_backend -> vieneu-tts-backend -> tts -> novel-reader
# Then: project_root/tts/VieNeu-TTS
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
VIENEU_REPO_PATH = PROJECT_ROOT / "tts" / "VieNeu-TTS"

if not VIENEU_REPO_PATH.exists():
    raise ImportError(
        f"VieNeu-TTS repository not found at: {VIENEU_REPO_PATH}\n"
        f"Repository VieNeu-TTS khÃ´ng tÃ¬m tháº¥y táº¡i: {VIENEU_REPO_PATH}\n"
        f"Expected location: tts/VieNeu-TTS relative to project root: {PROJECT_ROOT}"
    )

if str(VIENEU_REPO_PATH) not in sys.path:
    sys.path.insert(0, str(VIENEU_REPO_PATH))
    print(f"âœ… Added VieNeu-TTS repo to path: {VIENEU_REPO_PATH}")
    print(f"âœ… ÄÃ£ thÃªm repo VieNeu-TTS vÃ o path: {VIENEU_REPO_PATH}")

# Import EXACTLY like test_female_voice.py does (working example)
# Import CHÃNH XÃC nhÆ° test_female_voice.py lÃ m (vÃ­ dá»¥ hoáº¡t Ä‘á»™ng)
# No patches needed - we're using the same environment!
# KhÃ´ng cáº§n patch - chÃºng ta Ä‘ang sá»­ dá»¥ng cÃ¹ng mÃ´i trÆ°á»ng!
from vieneu_tts import VieNeuTTS

# Try to import config_local like the working test does
# Thá»­ import config_local nhÆ° test hoáº¡t Ä‘á»™ng lÃ m
try:
    from config_local import get_backbone_repo
    USE_LOCAL_CONFIG = True
except ImportError:
    USE_LOCAL_CONFIG = False
    # Fallback to our config system
    from config import ModelConfig


class VieNeuTTSWrapper:
    """
    Wrapper for VieNeu-TTS model / Wrapper cho model VieNeu-TTS
    
    This follows the exact initialization pattern from VieNeu-TTS repository examples.
    Class nÃ y tuÃ¢n theo Ä‘Ãºng pattern khá»Ÿi táº¡o tá»« cÃ¡c vÃ­ dá»¥ trong repository VieNeu-TTS.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize VieNeu-TTS model / Khá»Ÿi táº¡o model VieNeu-TTS
        
        Args:
            model_path: Path to local model directory / ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c model local
                       If None, uses default from config
                       Náº¿u None, sá»­ dá»¥ng máº·c Ä‘á»‹nh tá»« config
            device: Device to use (cuda/cpu/auto) / Thiáº¿t bá»‹ sá»­ dá»¥ng
                   If None, auto-detects (cuda if available, else cpu)
                   Náº¿u None, tá»± Ä‘á»™ng phÃ¡t hiá»‡n (cuda náº¿u cÃ³, khÃ´ng thÃ¬ cpu)
        """
        # Get model path EXACTLY like test_female_voice.py does
        # Láº¥y Ä‘Æ°á»ng dáº«n model CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
        if model_path:
            self.model_path = model_path
        elif USE_LOCAL_CONFIG:
            # Use config_local.get_backbone_repo() like the working test
            # Sá»­ dá»¥ng config_local.get_backbone_repo() nhÆ° test hoáº¡t Ä‘á»™ng
            self.model_path = get_backbone_repo()
        else:
            # Fallback to our config system
            self.model_path = ModelConfig.VIENEU_TTS["model_path"]
        
        # Determine device EXACTLY like test_female_voice.py does
        # XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹ CHÃNH XÃC nhÆ° test_female_voice.py lÃ m
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # Sample rate is always 24kHz for VieNeu-TTS
        # Táº§n sá»‘ láº¥y máº«u luÃ´n lÃ  24kHz cho VieNeu-TTS
        self.sample_rate = 24_000
        
        # Initialize model EXACTLY like test_female_voice.py does (working example)
        # Khá»Ÿi táº¡o model CHÃNH XÃC nhÆ° test_female_voice.py lÃ m (vÃ­ dá»¥ hoáº¡t Ä‘á»™ng)
        print(f"ðŸ–¥ï¸  Using device: {self.device}")
        print(f"ðŸ–¥ï¸  Sá»­ dá»¥ng thiáº¿t bá»‹: {self.device}")
        print(f"ðŸ“¦ Loading model from: {self.model_path}")
        print(f"ðŸ“¦ Äang táº£i model tá»«: {self.model_path}")
        
        # Initialize EXACTLY like test_female_voice.py: VieNeuTTS(...)
        # Khá»Ÿi táº¡o CHÃNH XÃC nhÆ° test_female_voice.py: VieNeuTTS(...)
        self.model = VieNeuTTS(
            backbone_repo=self.model_path,
            backbone_device=self.device,
            codec_repo="neuphonic/neucodec",
            codec_device=self.device
        )
        
        print("âœ… VieNeu-TTS loaded successfully")
        print("âœ… VieNeu-TTS Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng")
    
    def synthesize(
        self,
        text: str,
        ref_audio_path: str,
        ref_text: str,
        output_path: Optional[str] = None,
        max_chars: int = 256,
        auto_chunk: bool = True
    ) -> np.ndarray:
        """
        Synthesize speech / Tá»•ng há»£p giá»ng nÃ³i
        
        Supports long text generation by chunking (like infer_long_text.py).
        Há»— trá»£ táº¡o vÄƒn báº£n dÃ i báº±ng cÃ¡ch chia nhá» (nhÆ° infer_long_text.py).
        
        This follows the exact pattern from VieNeu-TTS repository examples.
        Function nÃ y tuÃ¢n theo Ä‘Ãºng pattern tá»« cÃ¡c vÃ­ dá»¥ trong repository VieNeu-TTS.
        
        Args:
            text: Input text / VÄƒn báº£n Ä‘áº§u vÃ o
            ref_audio_path: Path to reference audio / ÄÆ°á»ng dáº«n audio tham chiáº¿u
            ref_text: Reference text (must match the reference audio) / VÄƒn báº£n tham chiáº¿u (pháº£i khá»›p vá»›i audio tham chiáº¿u)
            output_path: Optional output path / ÄÆ°á»ng dáº«n Ä‘áº§u ra tÃ¹y chá»n
            max_chars: Maximum characters per chunk (default: 256) / KÃ½ tá»± tá»‘i Ä‘a má»—i chunk (máº·c Ä‘á»‹nh: 256)
            auto_chunk: Automatically chunk long text (default: True) / Tá»± Ä‘á»™ng chia nhá» vÄƒn báº£n dÃ i (máº·c Ä‘á»‹nh: True)
            
        Returns:
            Audio array (numpy array) / Máº£ng audio (numpy array)
        """
        # Import chunking utility / Import tiá»‡n Ã­ch chia nhá»
        import sys
        from pathlib import Path
        chunker_path = Path(__file__).parent.parent / "text_chunker.py"
        if str(chunker_path.parent) not in sys.path:
            sys.path.insert(0, str(chunker_path.parent))
        from text_chunker import split_text_into_chunks, should_chunk_text
        
        # Encode reference audio ONCE (reused for all chunks) / MÃ£ hÃ³a audio tham chiáº¿u Má»˜T Láº¦N (tÃ¡i sá»­ dá»¥ng cho táº¥t cáº£ chunks)
        ref_codes = self.model.encode_reference(ref_audio_path)
        
        # Check if text needs chunking / Kiá»ƒm tra xem vÄƒn báº£n cÃ³ cáº§n chia nhá» khÃ´ng
        if auto_chunk and should_chunk_text(text, max_chars):
            # Split into chunks / Chia thÃ nh chunks
            chunks = split_text_into_chunks(text, max_chars=max_chars)
            
            if not chunks:
                raise ValueError("Text could not be segmented into valid chunks")
            
            print(f"ðŸ“„ Long text detected: splitting into {len(chunks)} chunks (â‰¤{max_chars} chars each)")
            print(f"ðŸ“„ PhÃ¡t hiá»‡n vÄƒn báº£n dÃ i: chia thÃ nh {len(chunks)} chunks (â‰¤{max_chars} kÃ½ tá»± má»—i chunk)")
            
            # Generate audio for each chunk / Táº¡o audio cho má»—i chunk
            generated_segments = []
            for idx, chunk in enumerate(chunks, start=1):
                print(f"ðŸŽ™ï¸ Generating chunk {idx}/{len(chunks)} ({len(chunk)} chars) / Äang táº¡o chunk {idx}/{len(chunks)} ({len(chunk)} kÃ½ tá»±)")
                # Reuse same ref_codes for all chunks (key optimization!) / TÃ¡i sá»­ dá»¥ng cÃ¹ng ref_codes cho táº¥t cáº£ chunks (tá»‘i Æ°u quan trá»ng!)
                wav = self.model.infer(chunk, ref_codes, ref_text)
                generated_segments.append(wav)
            
            # Concatenate all segments / Ná»‘i táº¥t cáº£ cÃ¡c Ä‘oáº¡n
            combined_audio = np.concatenate(generated_segments)
            
            # Save if output path provided / LÆ°u náº¿u cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº§u ra
            if output_path:
                sf.write(output_path, combined_audio, self.sample_rate)
            
            print(f"âœ… Generated long text audio ({len(chunks)} chunks combined) / ÄÃ£ táº¡o audio vÄƒn báº£n dÃ i ({len(chunks)} chunks Ä‘Ã£ káº¿t há»£p)")
            return combined_audio
        else:
            # Short text - generate directly / VÄƒn báº£n ngáº¯n - táº¡o trá»±c tiáº¿p
            wav = self.model.infer(text, ref_codes, ref_text)
            
            # Save if output path provided / LÆ°u náº¿u cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº§u ra
            if output_path:
                sf.write(output_path, wav, self.sample_rate)
            
            return wav
    
    def get_sample_rate(self) -> int:
        """Get sample rate / Láº¥y táº§n sá»‘ láº¥y máº«u"""
        return self.sample_rate

