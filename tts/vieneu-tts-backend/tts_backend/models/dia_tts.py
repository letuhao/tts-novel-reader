"""
Dia TTS Model Wrapper
Wrapper cho Model Dia TTS
"""
import sys
from pathlib import Path
from typing import Optional
import torch
import numpy as np

# Add Dia repo to path
DIA_REPO_PATH = Path(__file__).parent.parent.parent.parent / "tts" / "Dia-Finetuning-Vietnamese"
sys.path.insert(0, str(DIA_REPO_PATH))

from dia.model import Dia as DiaModel
from ..config import ModelConfig


def trim_silence(audio: np.ndarray, threshold: float = 0.01, margin: int = 1000) -> np.ndarray:
    """
    Remove silence from the beginning and end of audio.
    Cáº¯t bá» vÃ¹ng im láº·ng á»Ÿ Ä‘áº§u vÃ  cuá»‘i audio.
    
    Improved algorithm: Only trims from start/end, never cuts content in middle.
    Thuáº­t toÃ¡n cáº£i tiáº¿n: Chá»‰ cáº¯t tá»« Ä‘áº§u/cuá»‘i, khÃ´ng bao giá» cáº¯t ná»™i dung á»Ÿ giá»¯a.
    
    Args:
        audio: Audio array / Máº£ng audio
        threshold: Amplitude threshold to consider as 'sound' / NgÆ°á»¡ng biÃªn Ä‘á»™ Ä‘á»ƒ coi lÃ  'cÃ³ tiáº¿ng'
        margin: Keep some samples before and after the sound region / Giá»¯ láº¡i má»™t Ã­t máº«u trÆ°á»›c vÃ  sau vÃ¹ng cÃ³ tiáº¿ng
        
    Returns:
        Trimmed audio array / Máº£ng audio Ä‘Ã£ cáº¯t
    """
    if audio.size == 0:
        return audio
    
    # Use envelope detection to handle quiet but valid speech
    # Sá»­ dá»¥ng phÃ¡t hiá»‡n envelope Ä‘á»ƒ xá»­ lÃ½ giá»ng nÃ³i yáº¿u nhÆ°ng há»£p lá»‡
    abs_audio = np.abs(audio)
    
    # Calculate windowed RMS (root mean square) for more robust detection
    # TÃ­nh RMS theo cá»­a sá»• Ä‘á»ƒ phÃ¡t hiá»‡n chÃ­nh xÃ¡c hÆ¡n
    # Use larger window to avoid cutting on brief pauses
    # Sá»­ dá»¥ng cá»­a sá»• lá»›n hÆ¡n Ä‘á»ƒ trÃ¡nh cáº¯t á»Ÿ khoáº£ng táº¡m dá»«ng ngáº¯n
    window_size = max(1000, int(0.02 * len(audio)))  # At least 1000 samples or 2% of audio
    if window_size > len(audio):
        window_size = len(audio)
    
    # Create sliding window RMS
    # Táº¡o RMS trÆ°á»£t
    window_half = window_size // 2
    rms_values = []
    
    for i in range(len(audio)):
        start_idx = max(0, i - window_half)
        end_idx = min(len(audio), i + window_half)
        window_audio = abs_audio[start_idx:end_idx]
        rms = np.sqrt(np.mean(window_audio ** 2))
        rms_values.append(rms)
    
    rms_array = np.array(rms_values)
    
    # Use RMS-based threshold (more robust than amplitude)
    # Sá»­ dá»¥ng ngÆ°á»¡ng dá»±a trÃªn RMS (cháº¯c cháº¯n hÆ¡n so vá»›i biÃªn Ä‘á»™)
    # Lower threshold to avoid cutting valid quiet speech
    # Giáº£m ngÆ°á»¡ng Ä‘á»ƒ trÃ¡nh cáº¯t giá»ng nÃ³i yáº¿u há»£p lá»‡
    rms_threshold = threshold * 0.5  # More lenient for RMS
    non_silent_indices = np.where(rms_array > rms_threshold)[0]
    
    if non_silent_indices.size == 0:
        # Completely silent, return as is / HoÃ n toÃ n im láº·ng, tráº£ vá» nhÆ° cÅ©
        return audio
    
    # Find start and end with margin / TÃ¬m Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cÃ³ margin
    # Only trim from actual start/end, never from middle
    # Chá»‰ cáº¯t tá»« Ä‘áº§u/cuá»‘i thá»±c táº¿, khÃ´ng bao giá» tá»« giá»¯a
    start = max(non_silent_indices[0] - margin, 0)
    end = min(non_silent_indices[-1] + margin + 1, len(audio))
    
    return audio[start:end]


def normalize_audio(audio: np.ndarray, target_db: float = -3.0, max_peak: float = 0.95) -> np.ndarray:
    """
    Normalize audio to target dB level and prevent clipping.
    Chuáº©n hÃ³a audio Ä‘áº¿n má»©c dB má»¥c tiÃªu vÃ  ngÄƒn cháº·n clipping.
    
    Args:
        audio: Audio array / Máº£ng audio
        target_db: Target dB level (negative value, e.g., -3.0 for -3dB) / Má»©c dB má»¥c tiÃªu
        max_peak: Maximum peak value to prevent clipping / GiÃ¡ trá»‹ peak tá»‘i Ä‘a Ä‘á»ƒ ngÄƒn clipping
        
    Returns:
        Normalized audio array / Máº£ng audio Ä‘Ã£ chuáº©n hÃ³a
    """
    if audio.size == 0:
        return audio
    
    # Get current max value / Láº¥y giÃ¡ trá»‹ max hiá»‡n táº¡i
    current_max = np.max(np.abs(audio))
    
    if current_max == 0:
        # Silent audio, return as is / Audio im láº·ng, tráº£ vá» nhÆ° cÅ©
        return audio
    
    # Prevent clipping first / NgÄƒn clipping trÆ°á»›c
    if current_max > max_peak:
        audio = audio * (max_peak / current_max)
        current_max = max_peak
    
    # Normalize to target dB if needed / Chuáº©n hÃ³a Ä‘áº¿n má»©c dB má»¥c tiÃªu náº¿u cáº§n
    if target_db is not None:
        # Convert dB to linear scale / Chuyá»ƒn dB sang tá»· lá»‡ tuyáº¿n tÃ­nh
        target_linear = 10 ** (target_db / 20.0)
        
        # Calculate scale factor / TÃ­nh há»‡ sá»‘ tá»· lá»‡
        scale_factor = target_linear / current_max
        
        # Ensure we don't clip / Äáº£m báº£o khÃ´ng clip
        if scale_factor * current_max > max_peak:
            scale_factor = max_peak / current_max
        
        # Apply normalization / Ãp dá»¥ng chuáº©n hÃ³a
        audio = audio * scale_factor
    
    return audio


def ensure_audio_format(audio: np.ndarray) -> np.ndarray:
    """
    Ensure audio is in correct format (float32, mono, clamped to [-1, 1]).
    Äáº£m báº£o audio á»Ÿ Ä‘á»‹nh dáº¡ng Ä‘Ãºng (float32, mono, giá»›i háº¡n trong [-1, 1]).
    
    Args:
        audio: Audio array / Máº£ng audio
        
    Returns:
        Formatted audio array / Máº£ng audio Ä‘Ã£ Ä‘á»‹nh dáº¡ng
    """
    if audio.size == 0:
        return audio
    
    # Convert to float32 if needed / Chuyá»ƒn sang float32 náº¿u cáº§n
    if audio.dtype != np.float32:
        if np.issubdtype(audio.dtype, np.integer):
            # Convert from integer to float / Chuyá»ƒn tá»« integer sang float
            max_val = np.iinfo(audio.dtype).max
            audio = audio.astype(np.float32) / max_val
        else:
            audio = audio.astype(np.float32)
    
    # Ensure mono (take first channel if stereo) / Äáº£m báº£o mono (láº¥y kÃªnh Ä‘áº§u náº¿u stereo)
    if audio.ndim > 1:
        if audio.shape[0] == 2:  # (channels, samples)
            audio = np.mean(audio, axis=0)
        elif audio.shape[1] == 2:  # (samples, channels)
            audio = np.mean(audio, axis=1)
        else:
            audio = audio.flatten()
    
    # Clamp to [-1, 1] range / Giá»›i háº¡n trong khoáº£ng [-1, 1]
    audio = np.clip(audio, -1.0, 1.0)
    
    return audio


class DiaTTSWrapper:
    """Wrapper for Dia TTS model / Wrapper cho model Dia TTS"""
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        checkpoint_path: Optional[str] = None,
        device: Optional[str] = None,
        use_half_precision: bool = True,  # Enable fp16 by default on CUDA for 2x speedup
        use_torch_compile: bool = True  # Enable torch.compile by default on CUDA for 20-30% speedup
    ):
        """
        Initialize Dia TTS model / Khá»Ÿi táº¡o model Dia TTS
        
        Args:
            config_path: Path to config file / ÄÆ°á»ng dáº«n file cáº¥u hÃ¬nh
            checkpoint_path: Path to checkpoint file / ÄÆ°á»ng dáº«n file checkpoint
            device: Device to use (cuda/cpu/auto) / Thiáº¿t bá»‹ sá»­ dá»¥ng
            use_half_precision: Use fp16 precision on CUDA for faster inference / Sá»­ dá»¥ng fp16 trÃªn CUDA Ä‘á»ƒ inference nhanh hÆ¡n
            use_torch_compile: Use torch.compile on CUDA for faster inference / Sá»­ dá»¥ng torch.compile trÃªn CUDA Ä‘á»ƒ inference nhanh hÆ¡n
        """
        self.config_path = config_path or ModelConfig.DIA["config_path"]
        self.checkpoint_path = checkpoint_path or ModelConfig.DIA["checkpoint_path"]
        self.device = device or (ModelConfig.DIA["device"] if torch.cuda.is_available() else "cpu")
        self.device_obj = torch.device(self.device)
        self.sample_rate = ModelConfig.DIA["sample_rate"]
        self.use_half_precision = use_half_precision
        self.use_torch_compile = use_torch_compile
        self.model = None
        self.max_safe_tokens = None  # Will be calculated after model load
        self._load_model()
    
    def _get_gpu_memory_info(self):
        """
        Get GPU memory information and calculate safe max_tokens.
        Láº¥y thÃ´ng tin bá»™ nhá»› GPU vÃ  tÃ­nh toÃ¡n max_tokens an toÃ n.
        
        Returns:
            Tuple of (total_vram_gb, free_vram_gb, max_safe_tokens)
        """
        if self.device_obj.type != "cuda":
            return None, None, None
        
        try:
            # Get GPU memory stats
            # Láº¥y thá»‘ng kÃª bá»™ nhá»› GPU
            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # GB
            reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)  # GB
            free_memory = total_memory - reserved_memory  # Available for new allocations
            
            # Estimate VRAM needed for model weights (Dia TTS ~3-4GB)
            # Æ¯á»›c tÃ­nh VRAM cáº§n cho model weights (Dia TTS ~3-4GB)
            model_base_memory = 4.0  # GB (conservative estimate)
            
            # Estimate VRAM needed per 1000 tokens during generation
            # Æ¯á»›c tÃ­nh VRAM cáº§n cho má»—i 1000 tokens trong lÃºc generation
            # Based on KV cache, decoder states, etc. (~0.5-1GB per 1000 tokens)
            # Dá»±a trÃªn KV cache, decoder states, v.v. (~0.5-1GB má»—i 1000 tokens)
            tokens_per_gb = 1000  # Conservative: 1000 tokens per GB
            
            # Calculate safe max tokens: use 70% of available free memory
            # TÃ­nh max tokens an toÃ n: dÃ¹ng 70% bá»™ nhá»› trá»‘ng cÃ³ sáºµn
            # Reserve 30% for PyTorch overhead, other allocations
            # Dá»± trá»¯ 30% cho overhead PyTorch, cÃ¡c cáº¥p phÃ¡t khÃ¡c
            available_for_tokens = free_memory - model_base_memory
            safe_memory_for_tokens = available_for_tokens * 0.7  # Use 70% of available
            
            max_safe_tokens = int(safe_memory_for_tokens * tokens_per_gb)
            
            # Round to nearest multiple of 128 (required by model)
            # LÃ m trÃ²n Ä‘áº¿n bá»™i sá»‘ gáº§n nháº¥t cá»§a 128 (yÃªu cáº§u cá»§a model)
            max_safe_tokens = (max_safe_tokens // 128) * 128
            
            # Set reasonable bounds
            # Äáº·t giá»›i háº¡n há»£p lÃ½
            min_max_tokens = 3072  # Minimum for reasonable audio length
            max_max_tokens = 12288  # Maximum to prevent OOM (hard limit)
            max_safe_tokens = max(min_max_tokens, min(max_safe_tokens, max_max_tokens))
            
            return total_memory, free_memory, max_safe_tokens
        except Exception as e:
            print(f"[DiaTTS] âš ï¸ Failed to get GPU memory info: {e}")
            return None, None, None
    
    def _setup_cuda_optimizations(self):
        """
        Setup CUDA optimizations (TF32, Flash Attention) as in original setup.
        Thiáº¿t láº­p tá»‘i Æ°u hÃ³a CUDA (TF32, Flash Attention) nhÆ° trong setup gá»‘c.
        """
        if self.device_obj.type != "cuda":
            return
        
        try:
            # Enable TF32: faster on Ampere+ while maintaining stability
            # Cho phÃ©p TF32: nhanh hÆ¡n trÃªn Ampere+ mÃ  váº«n á»•n Ä‘á»‹nh
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.set_float32_matmul_precision("high")
            print("[DiaTTS] âœ… TF32 optimizations enabled for CUDA")
            print("[DiaTTS] âœ… Tá»‘i Æ°u hÃ³a TF32 Ä‘Ã£ Ä‘Æ°á»£c báº­t cho CUDA")
        except Exception as e:
            print(f"[DiaTTS] âš ï¸ Failed to enable TF32: {e}")
        
        try:
            # Prioritize Flash Attention in SDPA if available
            # Æ¯u tiÃªn Flash-Attention trong SDPA náº¿u cÃ³
            from torch.nn.attention import sdpa_kernel
            sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=True)
            print("[DiaTTS] âœ… Flash Attention enabled for SDPA")
            print("[DiaTTS] âœ… Flash Attention Ä‘Ã£ Ä‘Æ°á»£c báº­t cho SDPA")
        except Exception as e:
            print(f"[DiaTTS] âš ï¸ Failed to enable Flash Attention: {e}")
    
    def _load_model(self):
        """Load Dia TTS model / Táº£i model Dia TTS"""
        print(f"[DiaTTS] Loading Dia TTS from: {self.checkpoint_path}")
        print(f"[DiaTTS] Äang táº£i Dia TTS tá»«: {self.checkpoint_path}")
        print(f"[DiaTTS] Device: {self.device}")
        print(f"[DiaTTS] Thiáº¿t bá»‹: {self.device}")
        
        # Setup CUDA optimizations (TF32, Flash Attention)
        # Thiáº¿t láº­p tá»‘i Æ°u hÃ³a CUDA (TF32, Flash Attention)
        self._setup_cuda_optimizations()
        
        # Load model from local checkpoint
        # Táº£i model tá»« checkpoint local
        self.model = DiaModel.from_local(
            config_path=self.config_path,
            checkpoint_path=self.checkpoint_path,
            device=self.device_obj
        )
        
        # Apply performance optimizations on CUDA (as in original setup)
        # Ãp dá»¥ng tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t trÃªn CUDA (nhÆ° trong setup gá»‘c)
        if self.device_obj.type == "cuda" and hasattr(self.model, "model"):
            # Apply half precision (fp16) BEFORE torch.compile to avoid dtype mismatches
            # Ãp dá»¥ng half precision (fp16) TRÆ¯á»šC torch.compile Ä‘á»ƒ trÃ¡nh lá»‡ch dtype
            # Note: Half precision and torch.compile can have dtype conflicts, so we use autocast instead
            # LÆ°u Ã½: Half precision vÃ  torch.compile cÃ³ thá»ƒ xung Ä‘á»™t dtype, nÃªn chÃºng ta dÃ¹ng autocast thay tháº¿
            if self.use_half_precision:
                try:
                    # Convert model to half precision (but not for torch.compile compatibility)
                    # Chuyá»ƒn model sang half precision (nhÆ°ng khÃ´ng Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch torch.compile)
                    # Actually, let's skip model.half() and use autocast during inference instead
                    # Thá»±c ra, hÃ£y bá» qua model.half() vÃ  dÃ¹ng autocast trong lÃºc inference thay tháº¿
                    print("[DiaTTS] â„¹ï¸ Half precision will be handled via autocast during inference")
                    print("[DiaTTS] â„¹ï¸ Half precision sáº½ Ä‘Æ°á»£c xá»­ lÃ½ qua autocast trong lÃºc inference")
                    # Store flag to use autocast during synthesis
                    # LÆ°u flag Ä‘á»ƒ dÃ¹ng autocast trong lÃºc synthesis
                    self._use_autocast_fp16 = True
                except Exception as e:
                    print(f"[DiaTTS] âš ï¸ Failed to setup half precision: {e}")
                    self._use_autocast_fp16 = False
            else:
                self._use_autocast_fp16 = False
            
            # Apply torch.compile for 20-30% additional speedup
            # Note: torch.compile works better with float32, use autocast for fp16 ops
            # Ãp dá»¥ng torch.compile Ä‘á»ƒ tÄƒng tá»‘c thÃªm 20-30%
            # LÆ°u Ã½: torch.compile hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vá»›i float32, dÃ¹ng autocast cho ops fp16
            if self.use_torch_compile:
                try:
                    # Only compile if not using model.half() (compiled models have dtype issues)
                    # Chá»‰ compile náº¿u khÃ´ng dÃ¹ng model.half() (model Ä‘Ã£ compile cÃ³ váº¥n Ä‘á» dtype)
                    if not self.use_half_precision:
                        self.model.model = torch.compile(self.model.model, backend="inductor")
                        print("[DiaTTS] âœ… torch.compile enabled (float32 mode)")
                        print("[DiaTTS] âœ… torch.compile Ä‘Ã£ Ä‘Æ°á»£c báº­t (cháº¿ Ä‘á»™ float32)")
                    else:
                        print("[DiaTTS] â„¹ï¸ torch.compile skipped (using autocast for fp16 instead)")
                        print("[DiaTTS] â„¹ï¸ torch.compile Ä‘Ã£ bá»‹ bá» qua (dÃ¹ng autocast cho fp16 thay tháº¿)")
                        self.use_torch_compile = False  # Disable to avoid conflicts
                except Exception as e:
                    print(f"[DiaTTS] âš ï¸ Failed to enable torch.compile: {e}")
                    self.use_torch_compile = False
        
        print("âœ… Dia TTS loaded successfully")
        print("âœ… Dia TTS Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng")
        
        # Calculate max safe tokens based on GPU memory after model is loaded
        # TÃ­nh toÃ¡n max safe tokens dá»±a trÃªn bá»™ nhá»› GPU sau khi model Ä‘Æ°á»£c táº£i
        if self.device_obj.type == "cuda":
            total_vram, free_vram, max_safe_tokens = self._get_gpu_memory_info()
            if max_safe_tokens:
                self.max_safe_tokens = max_safe_tokens
                print(f"[DiaTTS] ğŸ’¾ GPU Memory Info:")
                print(f"[DiaTTS] ğŸ’¾ ThÃ´ng tin Bá»™ nhá»› GPU:")
                print(f"[DiaTTS]    Total VRAM: {total_vram:.2f} GB")
                print(f"[DiaTTS]    Free VRAM: {free_vram:.2f} GB")
                print(f"[DiaTTS]    Max Safe Tokens: {max_safe_tokens} (~{max_safe_tokens/102:.1f}s)")
                print(f"[DiaTTS]    Tá»•ng VRAM: {total_vram:.2f} GB")
                print(f"[DiaTTS]    VRAM Trá»‘ng: {free_vram:.2f} GB")
                print(f"[DiaTTS]    Max Tokens An ToÃ n: {max_safe_tokens} (~{max_safe_tokens/102:.1f}s)")
            else:
                # Fallback to conservative default
                # Dá»± phÃ²ng vá» giÃ¡ trá»‹ máº·c Ä‘á»‹nh báº£o thá»§
                self.max_safe_tokens = 6144
                print(f"[DiaTTS] âš ï¸ Could not detect GPU memory, using conservative limit: {self.max_safe_tokens} tokens")
                print(f"[DiaTTS] âš ï¸ KhÃ´ng thá»ƒ phÃ¡t hiá»‡n bá»™ nhá»› GPU, dÃ¹ng giá»›i háº¡n báº£o thá»§: {self.max_safe_tokens} tokens")
    
    def synthesize(
        self,
        text: str,
        audio_prompt_path: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: float = 1.3,
        top_p: float = 0.95,
        cfg_scale: float = 3.0,
        use_cfg_filter: bool = True,
        cfg_filter_top_k: int = 35,
        speed_factor: float = 1.0,  # Normal speed (0.8-1.0, 1.0 = normal) / Tá»‘c Ä‘á»™ bÃ¬nh thÆ°á»ng
        trim_silence: bool = False,  # Trim silence from beginning and end / Cáº¯t im láº·ng á»Ÿ Ä‘áº§u vÃ  cuá»‘i
        silence_threshold: float = 0.005,  # Threshold for silence detection (lowered for better speech detection) / NgÆ°á»¡ng phÃ¡t hiá»‡n im láº·ng (giáº£m Ä‘á»ƒ phÃ¡t hiá»‡n giá»ng nÃ³i tá»‘t hÆ¡n)
        silence_margin: int = 2000,  # Margin in samples to keep (increased to preserve more) / Margin tÃ­nh báº±ng máº«u Ä‘á»ƒ giá»¯ láº¡i (tÄƒng Ä‘á»ƒ giá»¯ nhiá»u hÆ¡n)
        normalize: bool = True,  # Normalize audio levels / Chuáº©n hÃ³a má»©c audio
        normalize_target_db: float = -3.0,  # Target dB for normalization / Má»©c dB má»¥c tiÃªu cho chuáº©n hÃ³a
        max_peak: float = 0.95,  # Maximum peak to prevent clipping / Peak tá»‘i Ä‘a Ä‘á»ƒ ngÄƒn clipping
        output_path: Optional[str] = None
    ) -> np.ndarray:
        """
        Synthesize speech / Tá»•ng há»£p giá»ng nÃ³i
        
        Args:
            text: Input text with speaker tags (e.g., "[01] Your text") / VÄƒn báº£n Ä‘áº§u vÃ o cÃ³ tag ngÆ°á»i nÃ³i
            audio_prompt_path: Optional audio prompt path for voice cloning / ÄÆ°á»ng dáº«n audio prompt tÃ¹y chá»n
            max_tokens: Maximum audio tokens / Sá»‘ token audio tá»‘i Ä‘a
            temperature: Sampling temperature / Nhiá»‡t Ä‘á»™ láº¥y máº«u
            top_p: Nucleus sampling / Láº¥y máº«u nucleus
            cfg_scale: Classifier-free guidance scale / Tá»· lá»‡ hÆ°á»›ng dáº«n classifier-free
            use_cfg_filter: Use classifier-free guidance filter / Sá»­ dá»¥ng bá»™ lá»c classifier-free
            cfg_filter_top_k: Top-k for CFG filter / Top-k cho bá»™ lá»c CFG
            speed_factor: Speech speed factor (0.8-1.0, lower = slower) / Há»‡ sá»‘ tá»‘c Ä‘á»™ giá»ng nÃ³i
            trim_silence: Whether to trim silence from beginning and end / CÃ³ cáº¯t im láº·ng á»Ÿ Ä‘áº§u vÃ  cuá»‘i khÃ´ng
            silence_threshold: Threshold for silence detection (0.0-1.0) / NgÆ°á»¡ng phÃ¡t hiá»‡n im láº·ng
            silence_margin: Margin in samples to keep around sound / Margin tÃ­nh báº±ng máº«u Ä‘á»ƒ giá»¯ láº¡i xung quanh Ã¢m thanh
            normalize: Whether to normalize audio levels / CÃ³ chuáº©n hÃ³a má»©c audio khÃ´ng
            normalize_target_db: Target dB level for normalization / Má»©c dB má»¥c tiÃªu cho chuáº©n hÃ³a
            max_peak: Maximum peak value to prevent clipping / GiÃ¡ trá»‹ peak tá»‘i Ä‘a Ä‘á»ƒ ngÄƒn clipping
            output_path: Optional output path / ÄÆ°á»ng dáº«n Ä‘áº§u ra tÃ¹y chá»n
            
        Returns:
            Audio array / Máº£ng audio
        """
        # Normalize text: add period at end if missing (helps EOS detection for short sentences)
        # Chuáº©n hÃ³a text: thÃªm dáº¥u cháº¥m á»Ÿ cuá»‘i náº¿u thiáº¿u (giÃºp phÃ¡t hiá»‡n EOS cho cÃ¢u ngáº¯n)
        text = self._normalize_text_for_tts(text)
        
        # Calculate max_tokens based on text length if not provided
        # TÃ­nh max_tokens dá»±a trÃªn Ä‘á»™ dÃ i text náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        original_text_length = len(text)
        if max_tokens is None:
            # Estimate tokens needed based on text length
            # Æ¯á»›c tÃ­nh sá»‘ token cáº§n thiáº¿t dá»±a trÃªn Ä‘á»™ dÃ i text
            # Vietnamese speech: ~150-200 chars/second at normal speed
            # Tiáº¿ng Viá»‡t: ~150-200 kÃ½ tá»±/giÃ¢y á»Ÿ tá»‘c Ä‘á»™ bÃ¬nh thÆ°á»ng
            # Dia model: ~10-15 tokens/second of audio (rough estimate)
            # Model Dia: ~10-15 tokens/giÃ¢y audio (Æ°á»›c tÃ­nh thÃ´)
            # More conservative: ~20-25 tokens per character to ensure full generation
            # Báº£o thá»§ hÆ¡n: ~20-25 tokens má»—i kÃ½ tá»± Ä‘á»ƒ Ä‘áº£m báº£o táº¡o Ä‘áº§y Ä‘á»§
            
            # Estimate tokens: text_length * tokens_per_char (accounts for pauses, punctuation, etc.)
            # Æ¯á»›c tÃ­nh tokens: text_length * tokens_per_char (tÃ­nh Ä‘áº¿n táº¡m dá»«ng, dáº¥u cÃ¢u, v.v.)
            estimated_tokens = int(original_text_length * 20)  # ~20 tokens per char (conservative)
            
            # Minimum default: 3072 tokens (~30 seconds) - increased from config default 1536
            # Tá»‘i thiá»ƒu máº·c Ä‘á»‹nh: 3072 tokens (~30 giÃ¢y) - tÄƒng tá»« máº·c Ä‘á»‹nh config 1536
            # This ensures full dialog generation even for medium-length texts
            # Äiá»u nÃ y Ä‘áº£m báº£o táº¡o Ä‘áº§y Ä‘á»§ dialog ngay cáº£ cho vÄƒn báº£n Ä‘á»™ dÃ i trung bÃ¬nh
            default_max_tokens = 3072  # ~30 seconds of audio
            max_tokens = max(default_max_tokens, estimated_tokens)
            
            # Cap at GPU-safe maximum to prevent OOM
            # Giá»›i háº¡n á»Ÿ má»©c tá»‘i Ä‘a an toÃ n cho GPU Ä‘á»ƒ trÃ¡nh háº¿t bá»™ nhá»›
            if hasattr(self, 'max_safe_tokens') and self.max_safe_tokens:
                max_tokens = min(max_tokens, self.max_safe_tokens)
            else:
                # Fallback to conservative default if GPU memory info not available
                # Dá»± phÃ²ng vá» giÃ¡ trá»‹ máº·c Ä‘á»‹nh báº£o thá»§ náº¿u khÃ´ng cÃ³ thÃ´ng tin bá»™ nhá»› GPU
                max_tokens = min(max_tokens, 6144)
            
            print(f"[DiaTTS] Text length: {original_text_length} chars, estimated {estimated_tokens} tokens, using max_tokens: {max_tokens} (~{max_tokens/102:.1f}s)")
            print(f"[DiaTTS] Äá»™ dÃ i text: {original_text_length} kÃ½ tá»±, Æ°á»›c tÃ­nh {estimated_tokens} tokens, dÃ¹ng max_tokens: {max_tokens} (~{max_tokens/102:.1f}s)")
        else:
            # User provided max_tokens, but ensure it's at least the default minimum
            # NgÆ°á»i dÃ¹ng cung cáº¥p max_tokens, nhÆ°ng Ä‘áº£m báº£o Ã­t nháº¥t lÃ  tá»‘i thiá»ƒu máº·c Ä‘á»‹nh
            if max_tokens < 1536:
                print(f"[DiaTTS] âš ï¸ Provided max_tokens ({max_tokens}) is very low, using minimum 1536")
                print(f"[DiaTTS] âš ï¸ max_tokens Ä‘Æ°á»£c cung cáº¥p ({max_tokens}) ráº¥t tháº¥p, dÃ¹ng tá»‘i thiá»ƒu 1536")
                max_tokens = 1536
            print(f"[DiaTTS] Using provided max_tokens: {max_tokens} (~{max_tokens/102:.1f}s)")
            print(f"[DiaTTS] Sá»­ dá»¥ng max_tokens Ä‘Æ°á»£c cung cáº¥p: {max_tokens} (~{max_tokens/102:.1f}s)")
        
        # Generate speech / Táº¡o giá»ng nÃ³i
        # Use autocast for fp16 inference (safer than model.half() with torch.compile)
        # DÃ¹ng autocast cho inference fp16 (an toÃ n hÆ¡n model.half() vá»›i torch.compile)
        if hasattr(self, '_use_autocast_fp16') and self._use_autocast_fp16 and self.device_obj.type == "cuda":
            with torch.cuda.amp.autocast(dtype=torch.float16):
                wav = self.model.generate(
                    text=text,
                    max_tokens=max_tokens,
                    cfg_scale=cfg_scale,
                    temperature=temperature,
                    top_p=top_p,
                    use_cfg_filter=use_cfg_filter,
                    use_torch_compile=False,  # Don't use with autocast
                    cfg_filter_top_k=cfg_filter_top_k,
                    audio_prompt_path=audio_prompt_path
                )
        else:
            # Standard float32 inference (or autocast not enabled)
            # Inference float32 tiÃªu chuáº©n (hoáº·c autocast khÃ´ng Ä‘Æ°á»£c báº­t)
            wav = self.model.generate(
                text=text,
                max_tokens=max_tokens,
                cfg_scale=cfg_scale,
                temperature=temperature,
                top_p=top_p,
                use_cfg_filter=use_cfg_filter,
                use_torch_compile=self.use_torch_compile if self.device_obj.type == "cuda" else False,
                cfg_filter_top_k=cfg_filter_top_k,
                audio_prompt_path=audio_prompt_path
            )
        
        # Ensure audio format is correct / Äáº£m báº£o Ä‘á»‹nh dáº¡ng audio Ä‘Ãºng
        wav = ensure_audio_format(wav)
        
        # Get function references from module namespace to avoid parameter shadowing
        # Láº¥y tham chiáº¿u hÃ m tá»« module namespace Ä‘á»ƒ trÃ¡nh tham sá»‘ che khuáº¥t
        # Parameters with same names as functions shadow them, so access via module
        import sys
        module = sys.modules[__name__]
        _trim_silence_func = getattr(module, 'trim_silence')  # Get function from module
        _normalize_audio_func = getattr(module, 'normalize_audio')  # Get function from module
        
        # Trim silence from beginning and end / Cáº¯t im láº·ng á»Ÿ Ä‘áº§u vÃ  cuá»‘i
        # Parameter name 'trim_silence' (bool) shadows function 'trim_silence()'
        if trim_silence:  # Check boolean parameter
            original_length = len(wav)
            # Use function from module namespace (not shadowed by parameter)
            wav = _trim_silence_func(wav, threshold=silence_threshold, margin=silence_margin)
            trimmed_length = len(wav)
            if trimmed_length < original_length:
                trimmed_seconds = (original_length - trimmed_length) / self.sample_rate
                print(f"Trimmed silence: {original_length} -> {trimmed_length} samples ({trimmed_seconds:.2f}s removed)")
                print(f"ÄÃ£ cáº¯t im láº·ng: {original_length} -> {trimmed_length} máº«u ({trimmed_seconds:.2f}s Ä‘Ã£ loáº¡i bá»)")
        
        # Normalize audio levels / Chuáº©n hÃ³a má»©c audio
        # Parameter name 'normalize' (bool) would shadow function, but we use different name
        if normalize:  # Check boolean parameter
            # Use function from module namespace
            wav = _normalize_audio_func(wav, target_db=normalize_target_db, max_peak=max_peak)
        
        # Apply speed factor for slower narration / Ãp dá»¥ng há»‡ sá»‘ tá»‘c Ä‘á»™ cho narration cháº­m hÆ¡n
        if speed_factor < 1.0 and speed_factor >= 0.8:
            # Clamp speed factor to safe range / Giá»›i háº¡n há»‡ sá»‘ tá»‘c Ä‘á»™ trong khoáº£ng an toÃ n
            speed_factor = max(0.8, min(speed_factor, 1.0))
            
            # Resample audio to slow down speech / Láº¥y máº«u láº¡i audio Ä‘á»ƒ lÃ m cháº­m giá»ng nÃ³i
            original_len = len(wav)
            target_len = int(original_len / speed_factor)
            
            if target_len != original_len and target_len > 0:
                x_original = np.arange(original_len)
                x_resampled = np.linspace(0, original_len - 1, target_len)
                wav = np.interp(x_resampled, x_original, wav).astype(np.float32)
                print(f"Applied speed factor {speed_factor:.2f}x (slower): {original_len} -> {target_len} samples")
                print(f"ÄÃ£ Ã¡p dá»¥ng há»‡ sá»‘ tá»‘c Ä‘á»™ {speed_factor:.2f}x (cháº­m hÆ¡n): {original_len} -> {target_len} máº«u")
        
        # Save if output path provided / LÆ°u náº¿u cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº§u ra
        if output_path:
            import soundfile as sf
            sf.write(output_path, wav, self.sample_rate)
        
        return wav
    
    def _normalize_text_for_tts(self, text: str) -> str:
        """
        Normalize text for TTS generation to improve EOS detection.
        Chuáº©n hÃ³a text cho sinh TTS Ä‘á»ƒ cáº£i thiá»‡n phÃ¡t hiá»‡n EOS.
        
        Adds punctuation at end if missing, which helps the model
        detect end of speech for short sentences.
        
        ThÃªm dáº¥u cháº¥m á»Ÿ cuá»‘i náº¿u thiáº¿u, giÃºp model phÃ¡t hiá»‡n
        káº¿t thÃºc giá»ng nÃ³i cho cÃ¢u ngáº¯n.
        
        Args:
            text: Input text / VÄƒn báº£n Ä‘áº§u vÃ o
            
        Returns:
            Normalized text / VÄƒn báº£n Ä‘Ã£ chuáº©n hÃ³a
        """
        if not text or not text.strip():
            return text
        
        # Remove trailing whitespace
        # Loáº¡i bá» khoáº£ng tráº¯ng á»Ÿ cuá»‘i
        text = text.rstrip()
        
        # Vietnamese sentence-ending punctuation
        # Dáº¥u cÃ¢u káº¿t thÃºc cÃ¢u tiáº¿ng Viá»‡t
        sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']
        
        # Check if text ends with sentence-ending punctuation
        # Kiá»ƒm tra xem text cÃ³ káº¿t thÃºc báº±ng dáº¥u cÃ¢u káº¿t thÃºc cÃ¢u khÃ´ng
        ends_with_punctuation = any(text.endswith(punct) for punct in sentence_endings)
        
        # If missing punctuation, add period
        # Náº¿u thiáº¿u dáº¥u cÃ¢u, thÃªm dáº¥u cháº¥m
        if not ends_with_punctuation:
            text = text + '.'
        
        return text
    
    def get_sample_rate(self) -> int:
        """Get sample rate / Láº¥y táº§n sá»‘ láº¥y máº«u"""
        return self.sample_rate

