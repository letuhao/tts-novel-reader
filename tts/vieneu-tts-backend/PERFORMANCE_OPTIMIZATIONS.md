# VieNeu-TTS Performance Optimizations
# T·ªëi ∆∞u h√≥a Hi·ªáu su·∫•t VieNeu-TTS

## ‚úÖ Optimizations Applied / T·ªëi ∆∞u h√≥a ƒë√£ √Åp d·ª•ng

### 1. **TF32 (TensorFloat-32)**
- **Status:** ‚úÖ Enabled for RTX 4090 (Ampere architecture)
- **Tr·∫°ng th√°i:** ‚úÖ ƒê√£ b·∫≠t cho RTX 4090 (ki·∫øn tr√∫c Ampere)
- **Impact:** ~1.5-2x faster matrix multiplications
- **T√°c ƒë·ªông:** ~1.5-2x nhanh h∆°n cho c√°c ph√©p nh√¢n ma tr·∫≠n
- **Code:** `torch.backends.cuda.matmul.allow_tf32 = True`

### 2. **FP16 (Half Precision)**
- **Status:** ‚úÖ Enabled via autocast
- **Tr·∫°ng th√°i:** ‚úÖ ƒê√£ b·∫≠t qua autocast
- **Impact:** ~2x faster inference, ~50% less VRAM
- **T√°c ƒë·ªông:** ~2x nhanh h∆°n inference, ~50% √≠t VRAM h∆°n
- **Code:** `torch.cuda.amp.autocast(dtype=torch.float16)`

### 3. **torch.compile**
- **Status:** ‚úÖ Enabled for backbone model
- **Tr·∫°ng th√°i:** ‚úÖ ƒê√£ b·∫≠t cho backbone model
- **Impact:** ~20-30% speedup on top of other optimizations
- **T√°c ƒë·ªông:** ~20-30% tƒÉng t·ªëc th√™m v√†o c√°c t·ªëi ∆∞u h√≥a kh√°c
- **Code:** `torch.compile(model, backend="inductor", mode="reduce-overhead")`

### 4. **Flash Attention**
- **Status:** ‚úÖ Enabled if available
- **Tr·∫°ng th√°i:** ‚úÖ ƒê√£ b·∫≠t n·∫øu kh·∫£ d·ª•ng
- **Impact:** Faster attention operations, less memory
- **T√°c ƒë·ªông:** C√°c ph√©p to√°n attention nhanh h∆°n, √≠t b·ªô nh·ªõ h∆°n
- **Code:** `sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION])`

## üìä Expected Performance Improvements / C·∫£i thi·ªán Hi·ªáu su·∫•t Mong ƒë·ª£i

### Before Optimizations / Tr∆∞·ªõc T·ªëi ∆∞u h√≥a:
- GPU Utilization: 90%
- Speed: 2-4x slower than realtime
- **T·ªëc ƒë·ªô:** Ch·∫≠m h∆°n 2-4 l·∫ßn so v·ªõi realtime

### After Optimizations / Sau T·ªëi ∆∞u h√≥a:
- **TF32:** +50-100% faster matmul operations
- **FP16:** +100% faster inference, 50% less VRAM
- **torch.compile:** +20-30% additional speedup
- **Flash Attention:** +10-20% faster attention (if supported)
- **Combined:** Should reach **realtime or faster** on RTX 4090
- **T·ªïng h·ª£p:** N√™n ƒë·∫°t **realtime ho·∫∑c nhanh h∆°n** tr√™n RTX 4090

## ‚ö†Ô∏è Important Notes / L∆∞u √Ω Quan tr·ªçng

1. **First Call May Be Slow:** torch.compile requires a "warmup" call to compile the model
   - **L·∫ßn g·ªçi ƒë·∫ßu c√≥ th·ªÉ ch·∫≠m:** torch.compile c·∫ßn m·ªôt l·∫ßn "warmup" ƒë·ªÉ bi√™n d·ªãch model
   - This is normal - subsequent calls will be much faster
   - ƒêi·ªÅu n√†y l√† b√¨nh th∆∞·ªùng - c√°c l·∫ßn g·ªçi sau s·∫Ω nhanh h∆°n nhi·ªÅu

2. **FP16 Precision:** Half precision may cause minor quality differences
   - **FP16 Precision:** Half precision c√≥ th·ªÉ g√¢y kh√°c bi·ªát ch·∫•t l∆∞·ª£ng nh·ªè
   - Usually imperceptible for TTS
   - Th∆∞·ªùng kh√¥ng nh·∫≠n bi·∫øt ƒë∆∞·ª£c cho TTS

3. **Flash Attention:** Requires compatible PyTorch version and CUDA
   - **Flash Attention:** C·∫ßn phi√™n b·∫£n PyTorch v√† CUDA t∆∞∆°ng th√≠ch
   - Falls back to standard attention if not available
   - Quay v·ªÅ attention ti√™u chu·∫©n n·∫øu kh√¥ng kh·∫£ d·ª•ng

## üîß How to Disable Optimizations / C√°ch T·∫Øt T·ªëi ∆∞u h√≥a

If you encounter issues, you can disable optimizations:

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, b·∫°n c√≥ th·ªÉ t·∫Øt t·ªëi ∆∞u h√≥a:

```python
# In vieneu_tts.py, comment out:
# Trong vieneu_tts.py, comment out:

# if self.device == "cuda":
#     self._setup_cuda_optimizations()
#     self._apply_model_optimizations()
```

## üöÄ Usage / S·ª≠ d·ª•ng

Optimizations are **automatically enabled** when:
- Device is CUDA (GPU detected)
- PyTorch supports the features (torch.compile, Flash Attention)

T·ªëi ∆∞u h√≥a ƒë∆∞·ª£c **t·ª± ƒë·ªông b·∫≠t** khi:
- Device l√† CUDA (ph√°t hi·ªán GPU)
- PyTorch h·ªó tr·ª£ c√°c t√≠nh nƒÉng (torch.compile, Flash Attention)

No manual configuration needed!

Kh√¥ng c·∫ßn c·∫•u h√¨nh th·ªß c√¥ng!

