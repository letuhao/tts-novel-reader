"""
TTS Service - VietTTS backend service
D·ªãch v·ª• TTS - D·ªãch v·ª• backend VietTTS
"""
from typing import Optional, Literal, TYPE_CHECKING
import torch
import asyncio
import threading
import queue
import contextlib
import time
from datetime import datetime

if TYPE_CHECKING:
    from .models.viet_tts import VietTTSWrapper

from .config import ModelConfig

def detect_device() -> str:
    """
    Detect available device (cuda/cpu) / Ph√°t hi·ªán thi·∫øt b·ªã c√≥ s·∫µn (cuda/cpu)
    
    Checks both PyTorch CUDA and ONNX Runtime CUDA provider.
    Ki·ªÉm tra c·∫£ PyTorch CUDA v√† ONNX Runtime CUDA provider.
    
    Returns:
        "cuda" if GPU is available, "cpu" otherwise
        "cuda" n·∫øu GPU c√≥ s·∫µn, "cpu" n·∫øu kh√¥ng
    """
    # Check PyTorch CUDA first (for main models)
    # Ki·ªÉm tra PyTorch CUDA tr∆∞·ªõc (cho c√°c model ch√≠nh)
    if torch.cuda.is_available():
        return "cuda"
    
    # Check ONNX Runtime CUDA provider (for ONNX models)
    # Ki·ªÉm tra ONNX Runtime CUDA provider (cho c√°c model ONNX)
    try:
        import onnxruntime
        providers = onnxruntime.get_available_providers()
        if "CUDAExecutionProvider" in providers:
            print("‚ö†Ô∏è  PyTorch CUDA not available, but ONNX Runtime CUDA is available")
            print("‚ö†Ô∏è  PyTorch CUDA kh√¥ng kh·∫£ d·ª•ng, nh∆∞ng ONNX Runtime CUDA c√≥ s·∫µn")
            print("   PyTorch models will use CPU, ONNX models will use GPU")
            print("   Model PyTorch s·∫Ω d√πng CPU, model ONNX s·∫Ω d√πng GPU")
            # Still return "cuda" for ONNX parts, but note PyTorch limitation
            # V·∫´n tr·∫£ v·ªÅ "cuda" cho ph·∫ßn ONNX, nh∆∞ng l∆∞u √Ω gi·ªõi h·∫°n PyTorch
            return "cuda"
    except ImportError:
        pass
    
    return "cpu"

# Model types / Lo·∫°i model
ModelType = Literal["viet-tts"]

class ModelPool:
    """Model Pool for concurrent inference / Pool Model cho inference ƒë·ªìng th·ªùi"""
    
    def __init__(self, pool_size: int = 2, device: str = "cuda"):
        """
        Initialize model pool / Kh·ªüi t·∫°o pool model
        
        Args:
            pool_size: Number of model instances in pool / S·ªë l∆∞·ª£ng instance model trong pool
            device: Device to use (cuda/cpu) / Thi·∫øt b·ªã s·ª≠ d·ª•ng
        """
        self.pool_size = pool_size
        self.device = device
        self.pool = queue.Queue(maxsize=pool_size)
        self._lock = threading.Lock()
        self._initialized = False
        
    def _initialize_pool(self):
        """Initialize model instances in pool / Kh·ªüi t·∫°o c√°c instance model trong pool"""
        if self._initialized:
            return
            
        with self._lock:
            if self._initialized:
                return
                
            print(f"üîÑ Creating Model Pool with {self.pool_size} instances...")
            print(f"üîÑ ƒêang t·∫°o Model Pool v·ªõi {self.pool_size} instances...")
            
            for i in range(self.pool_size):
                print(f"   Loading model instance {i+1}/{self.pool_size}...")
                print(f"   ƒêang t·∫£i model instance {i+1}/{self.pool_size}...")
                from .models.viet_tts import VietTTSWrapper
                model = VietTTSWrapper(device=self.device)
                
                # Warmup if CUDA
                if self.device == "cuda":
                    print(f"   Warming up instance {i+1}/{self.pool_size}...")
                    print(f"   ƒêang l√†m n√≥ng instance {i+1}/{self.pool_size}...")
                    try:
                        model.warmup()
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Warmup failed for instance {i+1} (non-critical): {e}")
                
                self.pool.put(model)
                print(f"   ‚úÖ Instance {i+1}/{self.pool_size} ready")
                print(f"   ‚úÖ Instance {i+1}/{self.pool_size} s·∫µn s√†ng")
            
            self._initialized = True
            print(f"‚úÖ Model Pool initialized with {self.pool_size} instances")
            print(f"‚úÖ Model Pool ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi {self.pool_size} instances")
    
    @contextlib.contextmanager
    def get_model(self):
        """
        Get a model from pool (context manager) / L·∫•y m·ªôt model t·ª´ pool (context manager)
        
        Usage / C√°ch d√πng:
            with pool.get_model() as model:
                result = model.synthesize(...)
        """
        self._initialize_pool()  # Initialize on first use / Kh·ªüi t·∫°o khi d√πng l·∫ßn ƒë·∫ßu
        
        # Get model from pool (blocks if pool is empty)
        # L·∫•y model t·ª´ pool (block n·∫øu pool tr·ªëng)
        model = self.pool.get()
        
        try:
            yield model
        finally:
            # Return model to pool
            # Tr·∫£ model v·ªÅ pool
            self.pool.put(model)
    
    def get_pool_size(self) -> int:
        """Get pool size / L·∫•y k√≠ch th∆∞·ªõc pool"""
        return self.pool_size


class TTSService:
    """Unified TTS service / D·ªãch v·ª• TTS th·ªëng nh·∫•t"""
    
    def __init__(self, default_model: ModelType = "viet-tts", preload_default: bool = True, use_model_pool: bool = False, model_pool_size: int = 2):
        """
        Initialize TTS service / Kh·ªüi t·∫°o d·ªãch v·ª• TTS
        
        Args:
            default_model: Default model to use / Model m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng
            preload_default: Whether to preload default model at startup / C√≥ t·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh khi kh·ªüi ƒë·ªông kh√¥ng
            use_model_pool: Use model pool for concurrent inference / S·ª≠ d·ª•ng model pool cho inference ƒë·ªìng th·ªùi
            model_pool_size: Number of model instances in pool / S·ªë l∆∞·ª£ng instance model trong pool
        """
        self.default_model = default_model
        self.viet_tts = None
        self.device = detect_device()
        self.use_model_pool = use_model_pool and self.device == "cuda"  # Only use pool for GPU
        self.model_pool_size = model_pool_size
        
        # Model pool for concurrent inference / Pool model cho inference ƒë·ªìng th·ªùi
        if self.use_model_pool:
            self.model_pool = ModelPool(pool_size=model_pool_size, device=self.device)
            print(f"‚úÖ Using Model Pool with {model_pool_size} instances for concurrent inference")
            print(f"‚úÖ S·ª≠ d·ª•ng Model Pool v·ªõi {model_pool_size} instances cho inference ƒë·ªìng th·ªùi")
        else:
            self.model_pool = None
            # Thread lock for single model instance (fallback)
            # Kh√≥a thread cho instance model ƒë∆°n (d·ª± ph√≤ng)
            self._inference_lock = threading.Lock()
            print(f"‚ö†Ô∏è  Using single model instance with lock (sequential processing)")
            print(f"‚ö†Ô∏è  S·ª≠ d·ª•ng instance model ƒë∆°n v·ªõi lock (x·ª≠ l√Ω tu·∫ßn t·ª±)")
        print(f"Initializing TTS Service on device: {self.device}")
        print(f"Kh·ªüi t·∫°o D·ªãch v·ª• TTS tr√™n thi·∫øt b·ªã: {self.device}")
        
        # Show detailed device info
        # Hi·ªÉn th·ªã th√¥ng tin thi·∫øt b·ªã chi ti·∫øt
        if torch.cuda.is_available():
            print(f"‚úÖ PyTorch CUDA: Available (GPU: {torch.cuda.get_device_name(0)})")
            print(f"‚úÖ PyTorch CUDA: C√≥ s·∫µn (GPU: {torch.cuda.get_device_name(0)})")
        else:
            print("‚ö†Ô∏è  PyTorch CUDA: Not available (CPU-only PyTorch)")
            print("‚ö†Ô∏è  PyTorch CUDA: Kh√¥ng kh·∫£ d·ª•ng (PyTorch ch·ªâ CPU)")
        
        try:
            import onnxruntime
            # Safely check for available providers (handle partially uninstalled modules)
            # Ki·ªÉm tra providers c√≥ s·∫µn m·ªôt c√°ch an to√†n (x·ª≠ l√Ω module b·ªã g·ª° m·ªôt ph·∫ßn)
            if hasattr(onnxruntime, 'get_available_providers'):
                providers = onnxruntime.get_available_providers()
                if "CUDAExecutionProvider" in providers:
                    print(f"‚úÖ ONNX Runtime CUDA: Available (Providers: {providers})")
                    print(f"‚úÖ ONNX Runtime CUDA: C√≥ s·∫µn (Providers: {providers})")
                else:
                    print(f"‚ö†Ô∏è  ONNX Runtime CUDA: Not available (Providers: {providers})")
                    print(f"‚ö†Ô∏è  ONNX Runtime CUDA: Kh√¥ng kh·∫£ d·ª•ng (Providers: {providers})")
            else:
                print("‚ö†Ô∏è  ONNX Runtime: Module corrupted or incomplete (missing get_available_providers)")
                print("‚ö†Ô∏è  ONNX Runtime: Module b·ªã h·ªèng ho·∫∑c kh√¥ng ƒë·∫ßy ƒë·ªß (thi·∫øu get_available_providers)")
                print("   Please reinstall: pip install onnxruntime-gpu")
                print("   Vui l√≤ng c√†i ƒë·∫∑t l·∫°i: pip install onnxruntime-gpu")
        except ImportError:
            print("‚ö†Ô∏è  ONNX Runtime: Not installed")
            print("‚ö†Ô∏è  ONNX Runtime: Ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        except Exception as e:
            print(f"‚ö†Ô∏è  ONNX Runtime: Error checking providers: {e}")
            print(f"‚ö†Ô∏è  ONNX Runtime: L·ªói ki·ªÉm tra providers: {e}")
        print(f"Default model: {default_model}")
        print(f"Model m·∫∑c ƒë·ªãnh: {default_model}")
        
        # Preload default model at startup with warmup to eliminate 10s setup delay per request
        # T·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh khi kh·ªüi ƒë·ªông v·ªõi warmup ƒë·ªÉ lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup 10s m·ªói request
        if preload_default and not self.use_model_pool:
            print(f"Preloading default model: {default_model}...")
            print(f"ƒêang t·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh: {default_model}...")
            try:
                viet_tts = self.get_viet_tts()  # Preload VietTTS model
                print("‚úÖ VietTTS model preloaded to GPU")
                print("‚úÖ Model VietTTS ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc l√™n GPU")
                
                # Warmup to compile CUDA kernels once (eliminates 10s setup delay on each request)
                # Warmup ƒë·ªÉ compile CUDA kernels m·ªôt l·∫ßn (lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup 10s ·ªü m·ªói request)
                if self.device == "cuda":
                    print("üî• Warming up model (compiling CUDA kernels - eliminates 10s setup delay)...")
                    print("üî• ƒêang l√†m n√≥ng model (compile CUDA kernels - lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup 10s)...")
                    viet_tts.warmup(voice_name="quynh")  # Use default voice for warmup
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to preload VietTTS: {e}")
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫£i tr∆∞·ªõc VietTTS: {e}")
                import traceback
                traceback.print_exc()
            print("‚úÖ Default model ready (warmed up, CUDA kernels compiled)")
            print("‚úÖ Model m·∫∑c ƒë·ªãnh ƒë√£ s·∫µn s√†ng (ƒë√£ warmup, CUDA kernels ƒë√£ compile)")
        elif self.use_model_pool:
            print(f"‚ÑπÔ∏è  Model Pool will initialize lazily on first request (faster startup)")
            print(f"‚ÑπÔ∏è  Model Pool s·∫Ω kh·ªüi t·∫°o lazy ·ªü request ƒë·∫ßu ti√™n (kh·ªüi ƒë·ªông nhanh h∆°n)")
    
    def get_viet_tts(self):
        """Get or load VietTTS model / L·∫•y ho·∫∑c t·∫£i model VietTTS"""
        if self.viet_tts is None:
            print("Loading VietTTS model...")
            print("ƒêang t·∫£i model VietTTS...")
            from .models.viet_tts import VietTTSWrapper
            self.viet_tts = VietTTSWrapper(device=self.device)
        return self.viet_tts
    
    def synthesize(
        self,
        text: str,
        model: Optional[ModelType] = None,
        voice: Optional[str] = None,
        voice_file: Optional[str] = None,
        speed: float = 1.0,
        batch_chunks: Optional[int] = None,
        **kwargs
    ):
        """
        Synthesize speech using specified model / T·ªïng h·ª£p gi·ªçng n√≥i s·ª≠ d·ª•ng model ch·ªâ ƒë·ªãnh
        
        NOTE: This method supports concurrent inference via Model Pool (if enabled).
        L∆ØU √ù: Method n√†y h·ªó tr·ª£ inference ƒë·ªìng th·ªùi qua Model Pool (n·∫øu ƒë∆∞·ª£c b·∫≠t).
        
        Args:
            text: Input text / VƒÉn b·∫£n ƒë·∫ßu v√†o
            model: Model to use (viet-tts) / Model s·ª≠ d·ª•ng
            voice: Voice name from built-in voices / T√™n gi·ªçng t·ª´ gi·ªçng c√≥ s·∫µn
            voice_file: Path to custom voice file / ƒê∆∞·ªùng d·∫´n file gi·ªçng t√πy ch·ªânh
            speed: Speech speed (0.5-2.0, default: 1.0) / T·ªëc ƒë·ªô gi·ªçng n√≥i
            **kwargs: Additional model-specific parameters / Tham s·ªë b·ªï sung theo model
            
        Returns:
            Audio array / M·∫£ng audio
        """
        model = model or self.default_model
        
        if model != "viet-tts":
            raise ValueError(f"Unknown model: {model}")
        
        service_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [Service] Starting synthesize - Model: {model}, Voice: {voice or voice_file or 'default'}")
        print(f"[{timestamp}] [Service] B·∫Øt ƒë·∫ßu synthesize - Model: {model}, Gi·ªçng: {voice or voice_file or 'default'}")
        
        # Get model instance
        get_model_start = time.time()
        viet_tts = self.get_viet_tts()
        get_model_duration = time.time() - get_model_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [Service] Get model instance: {get_model_duration*1000:.2f}ms")
        print(f"[{timestamp}] [Service] L·∫•y instance model: {get_model_duration*1000:.2f}ms")
        
        # Call synthesize
        synthesize_start = time.time()
        result = viet_tts.synthesize(
            text=text,
            voice=voice,
            voice_file=voice_file,
            speed=speed,
            batch_chunks=batch_chunks,
            **kwargs
        )
        synthesize_duration = time.time() - synthesize_start
        service_total = time.time() - service_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        
        print(f"[{timestamp}] [Service] Synthesize call duration: {synthesize_duration:.3f}s")
        print(f"[{timestamp}] [Service] Th·ªùi gian g·ªçi synthesize: {synthesize_duration:.3f}s")
        print(f"[{timestamp}] [Service] Service total time: {service_total:.3f}s")
        print(f"[{timestamp}] [Service] T·ªïng th·ªùi gian service: {service_total:.3f}s")
        
        return result
    
    def get_model_info(self, model: ModelType) -> dict:
        """
        Get model information / L·∫•y th√¥ng tin model
        
        Args:
            model: Model type / Lo·∫°i model
            
        Returns:
            Model information dictionary / T·ª´ ƒëi·ªÉn th√¥ng tin model
        """
        if model == "viet-tts":
            viet_tts = self.get_viet_tts()
            return {
                "model": "DangVanSam VietTTS",
                "sample_rate": viet_tts.get_sample_rate(),
                "device": viet_tts.device,
                "requires_reference": False,  # Uses built-in voices or voice files
                "available_voices": list(viet_tts.list_voices().keys())
            }
        else:
            raise ValueError(f"Unknown model: {model}")

# Global service instance / Instance d·ªãch v·ª• to√†n c·ª•c
_service_instance: Optional[TTSService] = None

def get_service() -> TTSService:
    """Get global TTS service instance / L·∫•y instance d·ªãch v·ª• TTS to√†n c·ª•c"""
    global _service_instance
    if _service_instance is None:
        _service_instance = TTSService(default_model="viet-tts", preload_default=True)
    return _service_instance

