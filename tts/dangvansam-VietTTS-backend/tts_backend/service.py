"""
TTS Service - VietTTS backend service
D·ªãch v·ª• TTS - D·ªãch v·ª• backend VietTTS
"""
from typing import Optional, Literal, TYPE_CHECKING
import torch

if TYPE_CHECKING:
    from .models.viet_tts import VietTTSWrapper

from .config import ModelConfig

def detect_device() -> str:
    """
    Detect available device (cuda/cpu) / Ph√°t hi·ªán thi·∫øt b·ªã c√≥ s·∫µn (cuda/cpu)
    
    Checks both PyTorch CUDA and ONNX Runtime CUDA provider.
    Ki·ªÉm tra c·∫£ PyTorch CUDA v√† ONNX Runtime CUDA provider.
    
    Returns:
        "cuda" if GPU is available, "cpu" otherwise
        "cuda" n·∫øu GPU c√≥ s·∫µn, "cpu" n·∫øu kh√¥ng
    """
    # Check PyTorch CUDA first (for main models)
    # Ki·ªÉm tra PyTorch CUDA tr∆∞·ªõc (cho c√°c model ch√≠nh)
    if torch.cuda.is_available():
        return "cuda"
    
    # Check ONNX Runtime CUDA provider (for ONNX models)
    # Ki·ªÉm tra ONNX Runtime CUDA provider (cho c√°c model ONNX)
    try:
        import onnxruntime
        providers = onnxruntime.get_available_providers()
        if "CUDAExecutionProvider" in providers:
            print("‚ö†Ô∏è  PyTorch CUDA not available, but ONNX Runtime CUDA is available")
            print("‚ö†Ô∏è  PyTorch CUDA kh√¥ng kh·∫£ d·ª•ng, nh∆∞ng ONNX Runtime CUDA c√≥ s·∫µn")
            print("   PyTorch models will use CPU, ONNX models will use GPU")
            print("   Model PyTorch s·∫Ω d√πng CPU, model ONNX s·∫Ω d√πng GPU")
            # Still return "cuda" for ONNX parts, but note PyTorch limitation
            # V·∫´n tr·∫£ v·ªÅ "cuda" cho ph·∫ßn ONNX, nh∆∞ng l∆∞u √Ω gi·ªõi h·∫°n PyTorch
            return "cuda"
    except ImportError:
        pass
    
    return "cpu"

# Model types / Lo·∫°i model
ModelType = Literal["viet-tts"]

class TTSService:
    """Unified TTS service / D·ªãch v·ª• TTS th·ªëng nh·∫•t"""
    
    def __init__(self, default_model: ModelType = "viet-tts", preload_default: bool = True):
        """
        Initialize TTS service / Kh·ªüi t·∫°o d·ªãch v·ª• TTS
        
        Args:
            default_model: Default model to use / Model m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng
            preload_default: Whether to preload default model at startup / C√≥ t·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh khi kh·ªüi ƒë·ªông kh√¥ng
        """
        self.default_model = default_model
        self.viet_tts = None
        self.device = detect_device()
        print(f"Initializing TTS Service on device: {self.device}")
        print(f"Kh·ªüi t·∫°o D·ªãch v·ª• TTS tr√™n thi·∫øt b·ªã: {self.device}")
        
        # Show detailed device info
        # Hi·ªÉn th·ªã th√¥ng tin thi·∫øt b·ªã chi ti·∫øt
        if torch.cuda.is_available():
            print(f"‚úÖ PyTorch CUDA: Available (GPU: {torch.cuda.get_device_name(0)})")
            print(f"‚úÖ PyTorch CUDA: C√≥ s·∫µn (GPU: {torch.cuda.get_device_name(0)})")
        else:
            print("‚ö†Ô∏è  PyTorch CUDA: Not available (CPU-only PyTorch)")
            print("‚ö†Ô∏è  PyTorch CUDA: Kh√¥ng kh·∫£ d·ª•ng (PyTorch ch·ªâ CPU)")
        
        try:
            import onnxruntime
            providers = onnxruntime.get_available_providers()
            if "CUDAExecutionProvider" in providers:
                print(f"‚úÖ ONNX Runtime CUDA: Available (Providers: {providers})")
                print(f"‚úÖ ONNX Runtime CUDA: C√≥ s·∫µn (Providers: {providers})")
            else:
                print(f"‚ö†Ô∏è  ONNX Runtime CUDA: Not available (Providers: {providers})")
                print(f"‚ö†Ô∏è  ONNX Runtime CUDA: Kh√¥ng kh·∫£ d·ª•ng (Providers: {providers})")
        except ImportError:
            print("‚ö†Ô∏è  ONNX Runtime: Not installed")
            print("‚ö†Ô∏è  ONNX Runtime: Ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        print(f"Default model: {default_model}")
        print(f"Model m·∫∑c ƒë·ªãnh: {default_model}")
        
        # Preload default model at startup to avoid loading delay on first request
        # T·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh khi kh·ªüi ƒë·ªông ƒë·ªÉ tr√°nh ƒë·ªô tr·ªÖ t·∫£i ·ªü request ƒë·∫ßu ti√™n
        if preload_default:
            print(f"Preloading default model: {default_model}...")
            print(f"ƒêang t·∫£i tr∆∞·ªõc model m·∫∑c ƒë·ªãnh: {default_model}...")
            try:
                viet_tts = self.get_viet_tts()  # Preload VietTTS model
                print("‚úÖ VietTTS model preloaded to GPU")
                print("‚úÖ Model VietTTS ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc l√™n GPU")
                
                # Warmup model to prepare for fast inference
                # L√†m n√≥ng model ƒë·ªÉ chu·∫©n b·ªã cho inference nhanh
                if self.device == "cuda":
                    print("üî• Warming up model (this may take 30-60 seconds)...")
                    print("üî• ƒêang l√†m n√≥ng model (c√≥ th·ªÉ m·∫•t 30-60 gi√¢y)...")
                    viet_tts.warmup()
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to preload VietTTS: {e}")
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫£i tr∆∞·ªõc VietTTS: {e}")
                import traceback
                traceback.print_exc()
            print("‚úÖ Default model ready")
            print("‚úÖ Model m·∫∑c ƒë·ªãnh ƒë√£ s·∫µn s√†ng")
    
    def get_viet_tts(self):
        """Get or load VietTTS model / L·∫•y ho·∫∑c t·∫£i model VietTTS"""
        if self.viet_tts is None:
            print("Loading VietTTS model...")
            print("ƒêang t·∫£i model VietTTS...")
            from .models.viet_tts import VietTTSWrapper
            self.viet_tts = VietTTSWrapper(device=self.device)
        return self.viet_tts
    
    def synthesize(
        self,
        text: str,
        model: Optional[ModelType] = None,
        voice: Optional[str] = None,
        voice_file: Optional[str] = None,
        speed: float = 1.0,
        batch_chunks: Optional[int] = None,
        **kwargs
    ):
        """
        Synthesize speech using specified model / T·ªïng h·ª£p gi·ªçng n√≥i s·ª≠ d·ª•ng model ch·ªâ ƒë·ªãnh
        
        Args:
            text: Input text / VƒÉn b·∫£n ƒë·∫ßu v√†o
            model: Model to use (viet-tts) / Model s·ª≠ d·ª•ng
            voice: Voice name from built-in voices / T√™n gi·ªçng t·ª´ gi·ªçng c√≥ s·∫µn
            voice_file: Path to custom voice file / ƒê∆∞·ªùng d·∫´n file gi·ªçng t√πy ch·ªânh
            speed: Speech speed (0.5-2.0, default: 1.0) / T·ªëc ƒë·ªô gi·ªçng n√≥i
            **kwargs: Additional model-specific parameters / Tham s·ªë b·ªï sung theo model
            
        Returns:
            Audio array / M·∫£ng audio
        """
        model = model or self.default_model
        
        if model == "viet-tts":
            viet_tts = self.get_viet_tts()
            return viet_tts.synthesize(
                text=text,
                voice=voice,
                voice_file=voice_file,
                speed=speed,
                batch_chunks=batch_chunks,
                **kwargs
            )
        else:
            raise ValueError(f"Unknown model: {model}")
    
    def get_model_info(self, model: ModelType) -> dict:
        """
        Get model information / L·∫•y th√¥ng tin model
        
        Args:
            model: Model type / Lo·∫°i model
            
        Returns:
            Model information dictionary / T·ª´ ƒëi·ªÉn th√¥ng tin model
        """
        if model == "viet-tts":
            viet_tts = self.get_viet_tts()
            return {
                "model": "DangVanSam VietTTS",
                "sample_rate": viet_tts.get_sample_rate(),
                "device": viet_tts.device,
                "requires_reference": False,  # Uses built-in voices or voice files
                "available_voices": list(viet_tts.list_voices().keys())
            }
        else:
            raise ValueError(f"Unknown model: {model}")

# Global service instance / Instance d·ªãch v·ª• to√†n c·ª•c
_service_instance: Optional[TTSService] = None

def get_service() -> TTSService:
    """Get global TTS service instance / L·∫•y instance d·ªãch v·ª• TTS to√†n c·ª•c"""
    global _service_instance
    if _service_instance is None:
        _service_instance = TTSService(default_model="viet-tts", preload_default=True)
    return _service_instance

