"""
VietTTS Model Wrapper
Wrapper cho Model VietTTS

This wrapper uses the SAME environment as VietTTS for 100% compatibility.
Wrapper nÃ y sá»­ dá»¥ng CÃ™NG mÃ´i trÆ°á»ng vá»›i VietTTS Ä‘á»ƒ Ä‘áº£m báº£o 100% tÆ°Æ¡ng thÃ­ch.
"""
import sys
import warnings
import os
from pathlib import Path
from typing import Optional
import torch
import soundfile as sf
import numpy as np

# Suppress warnings
warnings.filterwarnings('ignore')
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# Add VietTTS repo to path FIRST (before any imports)
# File structure: tts/dangvansam-VietTTS-backend/tts_backend/models/viet_tts.py
# Go up 5 levels to project root: models -> tts_backend -> dangvansam-VietTTS-backend -> tts -> novel-reader
# Then: project_root/tts/viet-tts
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
VIETTTS_REPO_PATH = PROJECT_ROOT / "tts" / "viet-tts"

if not VIETTTS_REPO_PATH.exists():
    raise ImportError(
        f"VietTTS repository not found at: {VIETTTS_REPO_PATH}\n"
        f"Repository VietTTS khÃ´ng tÃ¬m tháº¥y táº¡i: {VIETTTS_REPO_PATH}\n"
        f"Expected location: tts/viet-tts relative to project root: {PROJECT_ROOT}"
    )

if str(VIETTTS_REPO_PATH) not in sys.path:
    sys.path.insert(0, str(VIETTTS_REPO_PATH))
    print(f"âœ… Added VietTTS repo to path: {VIETTTS_REPO_PATH}")
    print(f"âœ… ÄÃ£ thÃªm repo VietTTS vÃ o path: {VIETTTS_REPO_PATH}")

# Patch diffusers BEFORE importing viettts (fixes cached_download issue)
# Sá»­a diffusers TRÆ¯á»šC khi import viettts (sá»­a lá»—i cached_download)
# Find diffusers package location without importing it
def _patch_diffusers():
    """Patch diffusers to use hf_hub_download instead of cached_download"""
    try:
        # Find diffusers in site-packages without importing
        import site
        for site_packages in site.getsitepackages():
            diffusers_path = Path(site_packages) / "diffusers"
            dynamic_modules_path = diffusers_path / "utils" / "dynamic_modules_utils.py"
            
            if dynamic_modules_path.exists():
                content = dynamic_modules_path.read_text(encoding="utf-8")
                if "from huggingface_hub import cached_download, hf_hub_download, model_info" in content:
                    content = content.replace(
                        "from huggingface_hub import cached_download, hf_hub_download, model_info",
                        "from huggingface_hub import hf_hub_download, model_info"
                    )
                    content = content.replace("cached_download(", "hf_hub_download(")
                    dynamic_modules_path.write_text(content, encoding="utf-8")
                    print("âœ… Patched diffusers (cached_download -> hf_hub_download)")
                    print("âœ… ÄÃ£ sá»­a diffusers (cached_download -> hf_hub_download)")
                    return True
    except Exception:
        pass
    return False

# Apply patch before importing viettts
_patch_diffusers()

# Import VietTTS classes
from viettts.tts import TTS
from viettts.utils.file_utils import load_prompt_speech_from_file, load_voices


class VietTTSWrapper:
    """
    Wrapper for VietTTS model / Wrapper cho model VietTTS
    
    This follows the exact initialization pattern from VietTTS repository.
    Class nÃ y tuÃ¢n theo Ä‘Ãºng pattern khá»Ÿi táº¡o tá»« repository VietTTS.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize VietTTS model / Khá»Ÿi táº¡o model VietTTS
        
        Args:
            model_path: Path to local model directory / ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c model local
                       If None, uses default from config
                       Náº¿u None, sá»­ dá»¥ng máº·c Ä‘á»‹nh tá»« config
            device: Device to use (cuda/cpu/auto) / Thiáº¿t bá»‹ sá»­ dá»¥ng
                   If None, auto-detects (cuda if available, else cpu)
                   Náº¿u None, tá»± Ä‘á»™ng phÃ¡t hiá»‡n (cuda náº¿u cÃ³, khÃ´ng thÃ¬ cpu)
        """
        # Get model path
        if model_path:
            self.model_path = model_path
        else:
            # Use default from config
            from ..config import ModelConfig
            self.model_path = ModelConfig.VIETTTS["model_path"]
        
        # Determine device
        if device is None:
            # Use improved device detection that checks both PyTorch and ONNX Runtime
            # Sá»­ dá»¥ng phÃ¡t hiá»‡n thiáº¿t bá»‹ cáº£i tiáº¿n kiá»ƒm tra cáº£ PyTorch vÃ  ONNX Runtime
            from ..service import detect_device
            self.device = detect_device()
        else:
            self.device = device
        
        # Sample rate is 22,050 Hz for VietTTS
        self.sample_rate = 22_050
        
        # Get voice samples directory
        VOICE_SAMPLES_DIR = VIETTTS_REPO_PATH / "samples"
        self.voice_samples_dir = str(VOICE_SAMPLES_DIR)
        
        # Load available voices
        self.voice_map = load_voices(self.voice_samples_dir)
        
        # Initialize model
        print(f"ğŸ–¥ï¸  Using device: {self.device}")
        print(f"ğŸ–¥ï¸  Sá»­ dá»¥ng thiáº¿t bá»‹: {self.device}")
        print(f"ğŸ“¦ Loading model from: {self.model_path}")
        print(f"ğŸ“¦ Äang táº£i model tá»«: {self.model_path}")
        
        # Initialize VietTTS model
        self.model = TTS(
            model_dir=self.model_path,
            load_jit=False,  # Can enable for faster inference
            load_onnx=False  # Can enable for faster inference
        )
        
        # Apply performance optimizations for GPU
        if self.device == "cuda":
            self._setup_cuda_optimizations()
        
        print("âœ… VietTTS loaded successfully")
        print("âœ… VietTTS Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng")
    
    def warmup(self, voice_name: Optional[str] = None):
        """
        Warmup model with a dummy inference to prepare GPU for fast inference.
        LÃ m nÃ³ng model vá»›i inference giáº£ Ä‘á»ƒ chuáº©n bá»‹ GPU cho inference nhanh.
        
        Args:
            voice_name: Optional voice name for warmup / TÃªn giá»ng tÃ¹y chá»n Ä‘á»ƒ warmup
        """
        if self.device != "cuda":
            print("â„¹ï¸  Skipping warmup (CPU mode)")
            print("â„¹ï¸  Bá» qua warmup (cháº¿ Ä‘á»™ CPU)")
            return
        
        print("ğŸ”¥ Warming up model (preparing GPU for fast inference)...")
        print("ğŸ”¥ Äang lÃ m nÃ³ng model (chuáº©n bá»‹ GPU cho inference nhanh)...")
        
        try:
            # Use default voice if not provided
            if not voice_name:
                voice_name = "cdteam"  # Default voice
            
            # Get voice file
            voice_file = self.voice_map.get(voice_name)
            if not voice_file:
                voice_file = list(self.voice_map.values())[0]
            
            # Load voice
            prompt_speech = load_prompt_speech_from_file(voice_file)
            
            # Short dummy text for warmup
            dummy_text = "Xin chÃ o."
            
            # Perform a dummy inference to warmup the model and GPU
            print("   Running warmup inference (this may take 30-60 seconds)...")
            print("   Äang cháº¡y warmup inference (cÃ³ thá»ƒ máº¥t 30-60 giÃ¢y)...")
            
            _ = self.model.tts_to_wav(dummy_text, prompt_speech, speed=1.0)
            
            print("âœ… Model warmup completed!")
            print("âœ… Model warmup hoÃ n táº¥t!")
            print("   Model is now optimized and ready for fast inference!")
            print("   Model Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u vÃ  sáºµn sÃ ng cho inference nhanh!")
        except Exception as e:
            # Suppress WinError 193 warnings - it's handled by the frontend patch
            # ONNX Runtime will use CPU if CUDA DLL fails, but PyTorch models use GPU
            # áº¨n cáº£nh bÃ¡o WinError 193 - Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi frontend patch
            # ONNX Runtime sáº½ dÃ¹ng CPU náº¿u DLL CUDA tháº¥t báº¡i, nhÆ°ng model PyTorch dÃ¹ng GPU
            error_msg = str(e)
            if "193" not in error_msg and "WinError" not in error_msg:
                # Only show non-DLL errors
                # Chá»‰ hiá»ƒn thá»‹ lá»—i khÃ´ng pháº£i DLL
                print(f"âš ï¸  Warmup failed (non-critical): {e}")
                print(f"âš ï¸  Warmup tháº¥t báº¡i (khÃ´ng nghiÃªm trá»ng): {e}")
            # Model will still work - PyTorch uses GPU, ONNX uses CPU (handled by patch)
            # Model váº«n sáº½ hoáº¡t Ä‘á»™ng - PyTorch dÃ¹ng GPU, ONNX dÃ¹ng CPU (Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi patch)
    
    def _setup_cuda_optimizations(self):
        """
        Setup CUDA optimizations (TF32) for better performance.
        Thiáº¿t láº­p tá»‘i Æ°u hÃ³a CUDA (TF32) Ä‘á»ƒ hiá»‡u suáº¥t tá»‘t hÆ¡n.
        """
        try:
            # Enable TF32 for Ampere+ GPUs (RTX 4090 supports this)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            print("ğŸš€ CUDA optimizations enabled:")
            print("ğŸš€ Tá»‘i Æ°u hÃ³a CUDA Ä‘Ã£ Ä‘Æ°á»£c báº­t:")
            print("   - TF32 enabled for faster matmul operations")
            print("   - TF32 Ä‘Ã£ Ä‘Æ°á»£c báº­t cho cÃ¡c phÃ©p toÃ¡n matmul nhanh hÆ¡n")
        except Exception as e:
            print(f"âš ï¸  Warning: Could not enable all CUDA optimizations: {e}")
            print(f"âš ï¸  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ báº­t táº¥t cáº£ tá»‘i Æ°u hÃ³a CUDA: {e}")
    
    def synthesize(
        self,
        text: str,
        voice: Optional[str] = None,
        voice_file: Optional[str] = None,
        speed: float = 1.0,
        output_path: Optional[str] = None,
        batch_chunks: Optional[int] = None  # Process N chunks at a time to keep GPU busy
    ) -> np.ndarray:
        """
        Synthesize speech / Tá»•ng há»£p giá»ng nÃ³i
        
        Optimized version that pre-processes chunks and keeps GPU busy.
        PhiÃªn báº£n tá»‘i Æ°u pre-processes chunks vÃ  giá»¯ GPU báº­n.
        
        Args:
            text: Input text / VÄƒn báº£n Ä‘áº§u vÃ o
            voice: Voice name from built-in voices / TÃªn giá»ng tá»« giá»ng cÃ³ sáºµn
            voice_file: Path to custom voice file / ÄÆ°á»ng dáº«n file giá»ng tÃ¹y chá»‰nh
            speed: Speech speed (0.5-2.0, default: 1.0) / Tá»‘c Ä‘á»™ giá»ng nÃ³i
            output_path: Optional output path / ÄÆ°á»ng dáº«n Ä‘áº§u ra tÃ¹y chá»n
            batch_chunks: Process N chunks at a time to keep GPU busy (default: None = auto)
                         Xá»­ lÃ½ N chunks cÃ¹ng lÃºc Ä‘á»ƒ giá»¯ GPU báº­n (máº·c Ä‘á»‹nh: None = tá»± Ä‘á»™ng)
            
        Returns:
            Audio array (numpy array) / Máº£ng audio (numpy array)
        """
        # Determine voice file
        if voice_file:
            prompt_speech_file = voice_file
        elif voice:
            prompt_speech_file = self.voice_map.get(voice)
            if not prompt_speech_file:
                raise ValueError(f"Voice '{voice}' not found. Available voices: {list(self.voice_map.keys())}")
        else:
            # Use default voice
            prompt_speech_file = list(self.voice_map.values())[0]
        
        # Load prompt speech once
        prompt_speech = load_prompt_speech_from_file(prompt_speech_file)
        
        # Standard processing - VietTTS handles chunking internally
        # The batch_chunks parameter is reserved for future optimization
        # Xá»­ lÃ½ tiÃªu chuáº©n - VietTTS xá»­ lÃ½ chunking ná»™i bá»™
        # Tham sá»‘ batch_chunks Ä‘Æ°á»£c dÃ nh cho tá»‘i Æ°u hÃ³a tÆ°Æ¡ng lai
        wav = self.model.tts_to_wav(text, prompt_speech, speed=speed)
        
        # Save if output path provided
        if output_path:
            sf.write(output_path, wav, self.sample_rate)
        
        return wav
    
    def get_sample_rate(self) -> int:
        """Get sample rate / Láº¥y táº§n sá»‘ láº¥y máº«u"""
        return self.sample_rate
    
    def list_voices(self) -> dict:
        """
        List available voices / Liá»‡t kÃª cÃ¡c giá»ng cÃ³ sáºµn
        
        Returns:
            Dictionary mapping voice names to file paths / Tá»« Ä‘iá»ƒn Ã¡nh xáº¡ tÃªn giá»ng Ä‘áº¿n Ä‘Æ°á»ng dáº«n file
        """
        return self.voice_map.copy()

