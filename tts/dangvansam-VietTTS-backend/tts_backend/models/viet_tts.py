"""
VietTTS Model Wrapper
Wrapper cho Model VietTTS

This wrapper uses the SAME environment as VietTTS for 100% compatibility.
Wrapper n√†y s·ª≠ d·ª•ng C√ôNG m√¥i tr∆∞·ªùng v·ªõi VietTTS ƒë·ªÉ ƒë·∫£m b·∫£o 100% t∆∞∆°ng th√≠ch.
"""
import sys
import warnings
import os
from pathlib import Path
from typing import Optional
import torch
import soundfile as sf
import numpy as np

# Suppress warnings
warnings.filterwarnings('ignore')
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# Add VietTTS repo to path FIRST (before any imports)
# File structure: tts/dangvansam-VietTTS-backend/tts_backend/models/viet_tts.py
# Go up 5 levels to project root: models -> tts_backend -> dangvansam-VietTTS-backend -> tts -> novel-reader
# Then: project_root/tts/viet-tts
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
VIETTTS_REPO_PATH = PROJECT_ROOT / "tts" / "viet-tts"

if not VIETTTS_REPO_PATH.exists():
    raise ImportError(
        f"VietTTS repository not found at: {VIETTTS_REPO_PATH}\n"
        f"Repository VietTTS kh√¥ng t√¨m th·∫•y t·∫°i: {VIETTTS_REPO_PATH}\n"
        f"Expected location: tts/viet-tts relative to project root: {PROJECT_ROOT}"
    )

if str(VIETTTS_REPO_PATH) not in sys.path:
    sys.path.insert(0, str(VIETTTS_REPO_PATH))
    print(f"‚úÖ Added VietTTS repo to path: {VIETTTS_REPO_PATH}")
    print(f"‚úÖ ƒê√£ th√™m repo VietTTS v√†o path: {VIETTTS_REPO_PATH}")

# Patch diffusers BEFORE importing viettts (fixes cached_download issue)
# S·ª≠a diffusers TR∆Ø·ªöC khi import viettts (s·ª≠a l·ªói cached_download)
# Find diffusers package location without importing it
def _patch_diffusers():
    """Patch diffusers to use hf_hub_download instead of cached_download"""
    try:
        # Find diffusers in site-packages without importing
        import site
        for site_packages in site.getsitepackages():
            diffusers_path = Path(site_packages) / "diffusers"
            dynamic_modules_path = diffusers_path / "utils" / "dynamic_modules_utils.py"
            
            if dynamic_modules_path.exists():
                content = dynamic_modules_path.read_text(encoding="utf-8")
                if "from huggingface_hub import cached_download, hf_hub_download, model_info" in content:
                    content = content.replace(
                        "from huggingface_hub import cached_download, hf_hub_download, model_info",
                        "from huggingface_hub import hf_hub_download, model_info"
                    )
                    content = content.replace("cached_download(", "hf_hub_download(")
                    dynamic_modules_path.write_text(content, encoding="utf-8")
                    print("‚úÖ Patched diffusers (cached_download -> hf_hub_download)")
                    print("‚úÖ ƒê√£ s·ª≠a diffusers (cached_download -> hf_hub_download)")
                    return True
    except Exception:
        pass
    return False

# Apply patch before importing viettts
_patch_diffusers()

# Import VietTTS classes
from viettts.tts import TTS
from viettts.utils.file_utils import load_prompt_speech_from_file, load_voices


class VietTTSWrapper:
    """
    Wrapper for VietTTS model / Wrapper cho model VietTTS
    
    This follows the exact initialization pattern from VietTTS repository.
    Class n√†y tu√¢n theo ƒë√∫ng pattern kh·ªüi t·∫°o t·ª´ repository VietTTS.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize VietTTS model / Kh·ªüi t·∫°o model VietTTS
        
        Args:
            model_path: Path to local model directory / ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model local
                       If None, uses default from config
                       N·∫øu None, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh t·ª´ config
            device: Device to use (cuda/cpu/auto) / Thi·∫øt b·ªã s·ª≠ d·ª•ng
                   If None, auto-detects (cuda if available, else cpu)
                   N·∫øu None, t·ª± ƒë·ªông ph√°t hi·ªán (cuda n·∫øu c√≥, kh√¥ng th√¨ cpu)
        """
        # Get model path
        if model_path:
            self.model_path = model_path
        else:
            # Use default from config
            from ..config import ModelConfig
            self.model_path = ModelConfig.VIETTTS["model_path"]
        
        # Determine device
        if device is None:
            # Use improved device detection that checks both PyTorch and ONNX Runtime
            # S·ª≠ d·ª•ng ph√°t hi·ªán thi·∫øt b·ªã c·∫£i ti·∫øn ki·ªÉm tra c·∫£ PyTorch v√† ONNX Runtime
            from ..service import detect_device
            self.device = detect_device()
        else:
            self.device = device
        
        # Sample rate is 22,050 Hz for VietTTS
        self.sample_rate = 22_050
        
        # Get voice samples directory
        VOICE_SAMPLES_DIR = VIETTTS_REPO_PATH / "samples"
        self.voice_samples_dir = str(VOICE_SAMPLES_DIR)
        
        # Load available voices
        self.voice_map = load_voices(self.voice_samples_dir)
        
        # Initialize model
        print(f"üñ•Ô∏è  Using device: {self.device}")
        print(f"üñ•Ô∏è  S·ª≠ d·ª•ng thi·∫øt b·ªã: {self.device}")
        print(f"üì¶ Loading model from: {self.model_path}")
        print(f"üì¶ ƒêang t·∫£i model t·ª´: {self.model_path}")
        
        # Initialize VietTTS model
        self.model = TTS(
            model_dir=self.model_path,
            load_jit=False,  # Can enable for faster inference
            load_onnx=False  # Can enable for faster inference
        )
        
        # Apply performance optimizations for GPU
        if self.device == "cuda":
            self._setup_cuda_optimizations()
        
        print("‚úÖ VietTTS loaded successfully")
        print("‚úÖ VietTTS ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
    
    def warmup(self, voice_name: Optional[str] = None):
        """
        Warmup model with a dummy inference to prepare GPU for fast inference.
        L√†m n√≥ng model v·ªõi inference gi·∫£ ƒë·ªÉ chu·∫©n b·ªã GPU cho inference nhanh.
        
        Args:
            voice_name: Optional voice name for warmup / T√™n gi·ªçng t√πy ch·ªçn ƒë·ªÉ warmup
        """
        if self.device != "cuda":
            print("‚ÑπÔ∏è  Skipping warmup (CPU mode)")
            print("‚ÑπÔ∏è  B·ªè qua warmup (ch·∫ø ƒë·ªô CPU)")
            return
        
        print("üî• Warming up model (preparing GPU for fast inference)...")
        print("üî• ƒêang l√†m n√≥ng model (chu·∫©n b·ªã GPU cho inference nhanh)...")
        
        try:
            # Use default voice if not provided
            if not voice_name:
                voice_name = "cdteam"  # Default voice
            
            # Get voice file
            voice_file = self.voice_map.get(voice_name)
            if not voice_file:
                voice_file = list(self.voice_map.values())[0]
            
            # Load voice
            prompt_speech = load_prompt_speech_from_file(voice_file)
            
            # Short dummy text for warmup
            dummy_text = "Xin ch√†o."
            
            # Perform a dummy inference to warmup the model and GPU
            print("   Running warmup inference (this may take 30-60 seconds)...")
            print("   ƒêang ch·∫°y warmup inference (c√≥ th·ªÉ m·∫•t 30-60 gi√¢y)...")
            
            _ = self.model.tts_to_wav(dummy_text, prompt_speech, speed=1.0)
            
            print("‚úÖ Model warmup completed!")
            print("‚úÖ Model warmup ho√†n t·∫•t!")
            print("   Model is now optimized and ready for fast inference!")
            print("   Model ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u v√† s·∫µn s√†ng cho inference nhanh!")
        except Exception as e:
            # Suppress WinError 193 warnings - it's handled by the frontend patch
            # ONNX Runtime will use CPU if CUDA DLL fails, but PyTorch models use GPU
            # ·∫®n c·∫£nh b√°o WinError 193 - ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi frontend patch
            # ONNX Runtime s·∫Ω d√πng CPU n·∫øu DLL CUDA th·∫•t b·∫°i, nh∆∞ng model PyTorch d√πng GPU
            error_msg = str(e)
            if "193" not in error_msg and "WinError" not in error_msg:
                # Only show non-DLL errors
                # Ch·ªâ hi·ªÉn th·ªã l·ªói kh√¥ng ph·∫£i DLL
                print(f"‚ö†Ô∏è  Warmup failed (non-critical): {e}")
                print(f"‚ö†Ô∏è  Warmup th·∫•t b·∫°i (kh√¥ng nghi√™m tr·ªçng): {e}")
            # Model will still work - PyTorch uses GPU, ONNX uses CPU (handled by patch)
            # Model v·∫´n s·∫Ω ho·∫°t ƒë·ªông - PyTorch d√πng GPU, ONNX d√πng CPU (ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi patch)
    
    def _setup_cuda_optimizations(self):
        """
        Setup CUDA optimizations (TF32) for better performance.
        Thi·∫øt l·∫≠p t·ªëi ∆∞u h√≥a CUDA (TF32) ƒë·ªÉ hi·ªáu su·∫•t t·ªët h∆°n.
        """
        try:
            # Enable TF32 for Ampere+ GPUs (RTX 4090 supports this)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            print("üöÄ CUDA optimizations enabled:")
            print("üöÄ T·ªëi ∆∞u h√≥a CUDA ƒë√£ ƒë∆∞·ª£c b·∫≠t:")
            print("   - TF32 enabled for faster matmul operations")
            print("   - TF32 ƒë√£ ƒë∆∞·ª£c b·∫≠t cho c√°c ph√©p to√°n matmul nhanh h∆°n")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not enable all CUDA optimizations: {e}")
            print(f"‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng th·ªÉ b·∫≠t t·∫•t c·∫£ t·ªëi ∆∞u h√≥a CUDA: {e}")
    
    def synthesize(
        self,
        text: str,
        voice: Optional[str] = None,
        voice_file: Optional[str] = None,
        speed: float = 1.0,
        output_path: Optional[str] = None,
        batch_chunks: Optional[int] = None  # Process N chunks at a time to keep GPU busy
    ) -> np.ndarray:
        """
        Synthesize speech / T·ªïng h·ª£p gi·ªçng n√≥i
        
        Optimized version that pre-processes chunks and keeps GPU busy.
        Phi√™n b·∫£n t·ªëi ∆∞u pre-processes chunks v√† gi·ªØ GPU b·∫≠n.
        
        Args:
            text: Input text / VƒÉn b·∫£n ƒë·∫ßu v√†o
            voice: Voice name from built-in voices / T√™n gi·ªçng t·ª´ gi·ªçng c√≥ s·∫µn
            voice_file: Path to custom voice file / ƒê∆∞·ªùng d·∫´n file gi·ªçng t√πy ch·ªânh
            speed: Speech speed (0.5-2.0, default: 1.0) / T·ªëc ƒë·ªô gi·ªçng n√≥i
            output_path: Optional output path / ƒê∆∞·ªùng d·∫´n ƒë·∫ßu ra t√πy ch·ªçn
            batch_chunks: Process N chunks at a time to keep GPU busy (default: None = auto)
                         X·ª≠ l√Ω N chunks c√πng l√∫c ƒë·ªÉ gi·ªØ GPU b·∫≠n (m·∫∑c ƒë·ªãnh: None = t·ª± ƒë·ªông)
            
        Returns:
            Audio array (numpy array) / M·∫£ng audio (numpy array)
        """
        # Determine voice file
        if voice_file:
            prompt_speech_file = voice_file
        elif voice:
            prompt_speech_file = self.voice_map.get(voice)
            if not prompt_speech_file:
                raise ValueError(f"Voice '{voice}' not found. Available voices: {list(self.voice_map.keys())}")
        else:
            # Use default voice
            prompt_speech_file = list(self.voice_map.values())[0]
        
        # Load prompt speech once
        prompt_speech = load_prompt_speech_from_file(prompt_speech_file)
        
        # Validate text before processing / X√°c th·ª±c vƒÉn b·∫£n tr∆∞·ªõc khi x·ª≠ l√Ω
        text = text.strip() if isinstance(text, str) else str(text).strip()
        
        if not text or len(text) == 0:
            raise ValueError(
                f"Text is empty. Cannot generate audio from empty text."
            )
        
        # Check for meaningful content (at least 10 characters, not just punctuation)
        # Ki·ªÉm tra n·ªôi dung c√≥ nghƒ©a (√≠t nh·∫•t 10 k√Ω t·ª±, kh√¥ng ch·ªâ d·∫•u c√¢u)
        meaningful_text = ''.join(c for c in text if c.isalnum() or c.isspace()).strip()
        
        if len(text) < 10 or len(meaningful_text) < 5:
            raise ValueError(
                f"Text is too short or contains only punctuation (length: {len(text)}, meaningful: {len(meaningful_text)}). "
                f"Minimum length: 10 characters with at least 5 meaningful characters. "
                f"Text: '{text[:50] if text else 'None'}...'"
            )
        
        # Standard processing - VietTTS handles chunking internally
        # The batch_chunks parameter is reserved for future optimization
        # X·ª≠ l√Ω ti√™u chu·∫©n - VietTTS x·ª≠ l√Ω chunking n·ªôi b·ªô
        # Tham s·ªë batch_chunks ƒë∆∞·ª£c d√†nh cho t·ªëi ∆∞u h√≥a t∆∞∆°ng lai
        try:
            wav = self.model.tts_to_wav(text, prompt_speech, speed=speed)
        except ValueError as e:
            # Handle empty array concatenation error
            # X·ª≠ l√Ω l·ªói concatenate m·∫£ng r·ªóng
            if "need at least one array" in str(e).lower() or "concatenate" in str(e).lower():
                raise ValueError(
                    f"Text processing resulted in empty chunks. "
                    f"Text length: {len(text)} chars. "
                    f"Text preview: {text[:100]}... "
                    f"Original error: {e}"
                )
            raise
        
        # Validate output / X√°c th·ª±c ƒë·∫ßu ra
        if wav is None or len(wav) == 0:
            raise ValueError(
                f"Generated audio is empty. "
                f"Text length: {len(text)} chars. "
                f"Text preview: {text[:100]}..."
            )
        
        # Save if output path provided
        if output_path:
            sf.write(output_path, wav, self.sample_rate)
        
        return wav
    
    def get_sample_rate(self) -> int:
        """Get sample rate / L·∫•y t·∫ßn s·ªë l·∫•y m·∫´u"""
        return self.sample_rate
    
    def list_voices(self) -> dict:
        """
        List available voices / Li·ªát k√™ c√°c gi·ªçng c√≥ s·∫µn
        
        Returns:
            Dictionary mapping voice names to file paths / T·ª´ ƒëi·ªÉn √°nh x·∫° t√™n gi·ªçng ƒë·∫øn ƒë∆∞·ªùng d·∫´n file
        """
        return self.voice_map.copy()

