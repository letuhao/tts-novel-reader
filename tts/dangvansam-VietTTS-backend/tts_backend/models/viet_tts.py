"""
VietTTS Model Wrapper
Wrapper cho Model VietTTS

This wrapper uses the SAME environment as VietTTS for 100% compatibility.
Wrapper n√†y s·ª≠ d·ª•ng C√ôNG m√¥i tr∆∞·ªùng v·ªõi VietTTS ƒë·ªÉ ƒë·∫£m b·∫£o 100% t∆∞∆°ng th√≠ch.
"""
import sys
import warnings
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Optional
import torch
import soundfile as sf
import numpy as np

# Suppress warnings
warnings.filterwarnings('ignore')
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# Add VietTTS repo to path FIRST (before any imports)
# File structure: tts/dangvansam-VietTTS-backend/tts_backend/models/viet_tts.py
# Go up 5 levels to project root: models -> tts_backend -> dangvansam-VietTTS-backend -> tts -> novel-reader
# Then: project_root/tts/viet-tts
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
VIETTTS_REPO_PATH = PROJECT_ROOT / "tts" / "viet-tts"

if not VIETTTS_REPO_PATH.exists():
    raise ImportError(
        f"VietTTS repository not found at: {VIETTTS_REPO_PATH}\n"
        f"Repository VietTTS kh√¥ng t√¨m th·∫•y t·∫°i: {VIETTTS_REPO_PATH}\n"
        f"Expected location: tts/viet-tts relative to project root: {PROJECT_ROOT}"
    )

if str(VIETTTS_REPO_PATH) not in sys.path:
    sys.path.insert(0, str(VIETTTS_REPO_PATH))
    print(f"‚úÖ Added VietTTS repo to path: {VIETTTS_REPO_PATH}")
    print(f"‚úÖ ƒê√£ th√™m repo VietTTS v√†o path: {VIETTTS_REPO_PATH}")

# Patch diffusers BEFORE importing viettts (fixes cached_download issue)
# S·ª≠a diffusers TR∆Ø·ªöC khi import viettts (s·ª≠a l·ªói cached_download)
# Find diffusers package location without importing it
def _patch_diffusers():
    """Patch diffusers to use hf_hub_download instead of cached_download"""
    try:
        # Find diffusers in site-packages without importing
        import site
        for site_packages in site.getsitepackages():
            diffusers_path = Path(site_packages) / "diffusers"
            dynamic_modules_path = diffusers_path / "utils" / "dynamic_modules_utils.py"
            
            if dynamic_modules_path.exists():
                content = dynamic_modules_path.read_text(encoding="utf-8")
                if "from huggingface_hub import cached_download, hf_hub_download, model_info" in content:
                    content = content.replace(
                        "from huggingface_hub import cached_download, hf_hub_download, model_info",
                        "from huggingface_hub import hf_hub_download, model_info"
                    )
                    content = content.replace("cached_download(", "hf_hub_download(")
                    dynamic_modules_path.write_text(content, encoding="utf-8")
                    print("‚úÖ Patched diffusers (cached_download -> hf_hub_download)")
                    print("‚úÖ ƒê√£ s·ª≠a diffusers (cached_download -> hf_hub_download)")
                    return True
    except Exception:
        pass
    return False

# Apply patch before importing viettts
_patch_diffusers()

# Import VietTTS classes
from viettts.tts import TTS
from viettts.utils.file_utils import load_prompt_speech_from_file, load_voices


class VietTTSWrapper:
    """
    Wrapper for VietTTS model / Wrapper cho model VietTTS
    
    This follows the exact initialization pattern from VietTTS repository.
    Class n√†y tu√¢n theo ƒë√∫ng pattern kh·ªüi t·∫°o t·ª´ repository VietTTS.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize VietTTS model / Kh·ªüi t·∫°o model VietTTS
        
        Args:
            model_path: Path to local model directory / ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model local
                       If None, uses default from config
                       N·∫øu None, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh t·ª´ config
            device: Device to use (cuda/cpu/auto) / Thi·∫øt b·ªã s·ª≠ d·ª•ng
                   If None, auto-detects (cuda if available, else cpu)
                   N·∫øu None, t·ª± ƒë·ªông ph√°t hi·ªán (cuda n·∫øu c√≥, kh√¥ng th√¨ cpu)
        """
        # Get model path
        if model_path:
            self.model_path = model_path
        else:
            # Use default from config
            from ..config import ModelConfig
            self.model_path = ModelConfig.VIETTTS["model_path"]
        
        # Determine device
        if device is None:
            # Use improved device detection that checks both PyTorch and ONNX Runtime
            # S·ª≠ d·ª•ng ph√°t hi·ªán thi·∫øt b·ªã c·∫£i ti·∫øn ki·ªÉm tra c·∫£ PyTorch v√† ONNX Runtime
            from ..service import detect_device
            self.device = detect_device()
        else:
            self.device = device
        
        # Sample rate is 22,050 Hz for VietTTS
        self.sample_rate = 22_050
        
        # Get voice samples directory
        VOICE_SAMPLES_DIR = VIETTTS_REPO_PATH / "samples"
        self.voice_samples_dir = str(VOICE_SAMPLES_DIR)
        
        # Load available voices
        self.voice_map = load_voices(self.voice_samples_dir)
        
        # Cache for loaded prompt speech (to avoid reloading from disk each time)
        # Cache cho prompt speech ƒë√£ t·∫£i (ƒë·ªÉ tr√°nh t·∫£i l·∫°i t·ª´ disk m·ªói l·∫ßn)
        self._prompt_speech_cache = {}
        
        # Initialize model
        print(f"üñ•Ô∏è  Using device: {self.device}")
        print(f"üñ•Ô∏è  S·ª≠ d·ª•ng thi·∫øt b·ªã: {self.device}")
        print(f"üì¶ Loading model from: {self.model_path}")
        print(f"üì¶ ƒêang t·∫£i model t·ª´: {self.model_path}")
        
        # Initialize VietTTS model
        self.model = TTS(
            model_dir=self.model_path,
            load_jit=False,  # Can enable for faster inference
            load_onnx=False  # Can enable for faster inference
        )
        
        # Apply performance optimizations for GPU
        if self.device == "cuda":
            self._setup_cuda_optimizations()
        
        print("‚úÖ VietTTS loaded successfully")
        print("‚úÖ VietTTS ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
        
        # Preload common voices to avoid disk I/O delay on first use
        # T·∫£i tr∆∞·ªõc c√°c gi·ªçng ph·ªï bi·∫øn ƒë·ªÉ tr√°nh ƒë·ªô tr·ªÖ I/O disk khi d√πng l·∫ßn ƒë·∫ßu
        self._preload_common_voices()
    
    def _preload_common_voices(self):
        """
        Preload common voices to avoid disk I/O delay on first use.
        T·∫£i tr∆∞·ªõc c√°c gi·ªçng ph·ªï bi·∫øn ƒë·ªÉ tr√°nh ƒë·ªô tr·ªÖ I/O disk khi d√πng l·∫ßn ƒë·∫ßu.
        """
        common_voices = ["quynh", "cdteam", "nu-nhe-nhang"]  # Most commonly used voices
        print("üì¶ Preloading common voices to memory...")
        print("üì¶ ƒêang t·∫£i tr∆∞·ªõc c√°c gi·ªçng ph·ªï bi·∫øn v√†o memory...")
        
        for voice_name in common_voices:
            if voice_name in self.voice_map:
                try:
                    voice_file = self.voice_map[voice_name]
                    if voice_name not in self._prompt_speech_cache:
                        self._prompt_speech_cache[voice_name] = load_prompt_speech_from_file(voice_file)
                        print(f"   ‚úÖ Preloaded voice: {voice_name}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Failed to preload voice {voice_name}: {e}")
        
        print("‚úÖ Common voices preloaded")
        print("‚úÖ C√°c gi·ªçng ph·ªï bi·∫øn ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc")
    
    def warmup(self, voice_name: Optional[str] = None):
        """
        Warmup model with a dummy inference to prepare GPU for fast inference.
        This compiles CUDA kernels once at startup, eliminating the 10s setup delay per request.
        L√†m n√≥ng model v·ªõi inference gi·∫£ ƒë·ªÉ chu·∫©n b·ªã GPU cho inference nhanh.
        ƒêi·ªÅu n√†y compile CUDA kernels m·ªôt l·∫ßn khi kh·ªüi ƒë·ªông, lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup 10s m·ªói request.
        
        Args:
            voice_name: Optional voice name for warmup / T√™n gi·ªçng t√πy ch·ªçn ƒë·ªÉ warmup
        """
        if self.device != "cuda":
            print("‚ÑπÔ∏è  Skipping warmup (CPU mode)")
            print("‚ÑπÔ∏è  B·ªè qua warmup (ch·∫ø ƒë·ªô CPU)")
            return
        
        print("üî• Warming up model (compiling CUDA kernels - this eliminates 10s setup delay per request)...")
        print("üî• ƒêang l√†m n√≥ng model (compile CUDA kernels - ƒëi·ªÅu n√†y lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup 10s m·ªói request)...")
        
        try:
            # Use default voice if not provided
            if not voice_name:
                voice_name = "quynh"  # Default voice for novel reader
            
            # Get voice file
            voice_file = self.voice_map.get(voice_name)
            if not voice_file:
                voice_file = list(self.voice_map.values())[0]
            
            # Load voice (will be cached)
            # T·∫£i gi·ªçng (s·∫Ω ƒë∆∞·ª£c cache)
            cache_key = voice_name if voice_name in self.voice_map else "default"
            if cache_key not in self._prompt_speech_cache:
                self._prompt_speech_cache[cache_key] = load_prompt_speech_from_file(voice_file)
            prompt_speech = self._prompt_speech_cache[cache_key]
            
            # Short dummy text for warmup
            dummy_text = "Xin ch√†o."
            
            # Perform a dummy inference to warmup the model and GPU
            # This compiles CUDA kernels once, eliminating setup delay on subsequent requests
            # Th·ª±c hi·ªán inference gi·∫£ ƒë·ªÉ warmup model v√† GPU
            # ƒêi·ªÅu n√†y compile CUDA kernels m·ªôt l·∫ßn, lo·∫°i b·ªè ƒë·ªô tr·ªÖ setup ·ªü c√°c request ti·∫øp theo
            print("   Running warmup inference (compiling CUDA kernels - one-time cost)...")
            print("   ƒêang ch·∫°y warmup inference (compile CUDA kernels - chi ph√≠ m·ªôt l·∫ßn)...")
            
            _ = self.model.tts_to_wav(dummy_text, prompt_speech, speed=1.0)
            
            print("‚úÖ Model warmup completed! CUDA kernels compiled - no more 10s setup delay!")
            print("‚úÖ Model warmup ho√†n t·∫•t! CUDA kernels ƒë√£ compile - kh√¥ng c√≤n ƒë·ªô tr·ªÖ setup 10s!")
            print("   Subsequent requests will be fast (near real-time)")
            print("   C√°c request ti·∫øp theo s·∫Ω nhanh (g·∫ßn real-time)")
        except Exception as e:
            # Suppress WinError 193 warnings - it's handled by the frontend patch
            # ONNX Runtime will use CPU if CUDA DLL fails, but PyTorch models use GPU
            # ·∫®n c·∫£nh b√°o WinError 193 - ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi frontend patch
            # ONNX Runtime s·∫Ω d√πng CPU n·∫øu DLL CUDA th·∫•t b·∫°i, nh∆∞ng model PyTorch d√πng GPU
            error_msg = str(e)
            if "193" not in error_msg and "WinError" not in error_msg:
                # Only show non-DLL errors
                # Ch·ªâ hi·ªÉn th·ªã l·ªói kh√¥ng ph·∫£i DLL
                print(f"‚ö†Ô∏è  Warmup failed (non-critical): {e}")
                print(f"‚ö†Ô∏è  Warmup th·∫•t b·∫°i (kh√¥ng nghi√™m tr·ªçng): {e}")
            # Model will still work - PyTorch uses GPU, ONNX uses CPU (handled by patch)
            # Model v·∫´n s·∫Ω ho·∫°t ƒë·ªông - PyTorch d√πng GPU, ONNX d√πng CPU (ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi patch)
    
    def _setup_cuda_optimizations(self):
        """
        Setup CUDA optimizations (TF32) for better performance.
        Thi·∫øt l·∫≠p t·ªëi ∆∞u h√≥a CUDA (TF32) ƒë·ªÉ hi·ªáu su·∫•t t·ªët h∆°n.
        """
        try:
            # Enable TF32 for Ampere+ GPUs (RTX 4090 supports this)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            print("üöÄ CUDA optimizations enabled:")
            print("üöÄ T·ªëi ∆∞u h√≥a CUDA ƒë√£ ƒë∆∞·ª£c b·∫≠t:")
            print("   - TF32 enabled for faster matmul operations")
            print("   - TF32 ƒë√£ ƒë∆∞·ª£c b·∫≠t cho c√°c ph√©p to√°n matmul nhanh h∆°n")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not enable all CUDA optimizations: {e}")
            print(f"‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng th·ªÉ b·∫≠t t·∫•t c·∫£ t·ªëi ∆∞u h√≥a CUDA: {e}")
    
    def synthesize(
        self,
        text: str,
        voice: Optional[str] = None,
        voice_file: Optional[str] = None,
        speed: float = 1.0,
        output_path: Optional[str] = None,
        batch_chunks: Optional[int] = None  # Process N chunks at a time to keep GPU busy
    ) -> np.ndarray:
        """
        Synthesize speech / T·ªïng h·ª£p gi·ªçng n√≥i
        
        Optimized version that pre-processes chunks and keeps GPU busy.
        Phi√™n b·∫£n t·ªëi ∆∞u pre-processes chunks v√† gi·ªØ GPU b·∫≠n.
        
        Args:
            text: Input text / VƒÉn b·∫£n ƒë·∫ßu v√†o
            voice: Voice name from built-in voices / T√™n gi·ªçng t·ª´ gi·ªçng c√≥ s·∫µn
            voice_file: Path to custom voice file / ƒê∆∞·ªùng d·∫´n file gi·ªçng t√πy ch·ªânh
            speed: Speech speed (0.5-2.0, default: 1.0) / T·ªëc ƒë·ªô gi·ªçng n√≥i
            output_path: Optional output path / ƒê∆∞·ªùng d·∫´n ƒë·∫ßu ra t√πy ch·ªçn
            batch_chunks: Process N chunks at a time to keep GPU busy (default: None = auto)
                         X·ª≠ l√Ω N chunks c√πng l√∫c ƒë·ªÉ gi·ªØ GPU b·∫≠n (m·∫∑c ƒë·ªãnh: None = t·ª± ƒë·ªông)
            
        Returns:
            Audio array (numpy array) / M·∫£ng audio (numpy array)
        """
        total_start = time.time()
        print(f"\n{'='*60}")
        print(f"[PERF] Starting synthesis - Text length: {len(text)} chars")
        print(f"[PERF] B·∫Øt ƒë·∫ßu synthesis - ƒê·ªô d√†i text: {len(text)} k√Ω t·ª±")
        print(f"{'='*60}")
        
        # Step 1: Determine voice file / B∆∞·ªõc 1: X√°c ƒë·ªãnh file gi·ªçng
        step_start = time.time()
        if voice_file:
            prompt_speech_file = voice_file
            cache_key = voice_file
        elif voice:
            prompt_speech_file = self.voice_map.get(voice)
            if not prompt_speech_file:
                raise ValueError(f"Voice '{voice}' not found. Available voices: {list(self.voice_map.keys())}")
            cache_key = voice
        else:
            # Use default voice
            prompt_speech_file = list(self.voice_map.values())[0]
            cache_key = "default"
        step_duration = time.time() - step_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 1 - Voice selection: {step_duration*1000:.2f}ms")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 1 - Ch·ªçn gi·ªçng: {step_duration*1000:.2f}ms")
        
        # Step 2: Load prompt speech from cache / B∆∞·ªõc 2: T·∫£i prompt speech t·ª´ cache
        step_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        if cache_key not in self._prompt_speech_cache:
            print(f"[{timestamp}] [PERF] Voice '{cache_key}' not in cache, loading from disk...")
            print(f"[{timestamp}] [PERF] Gi·ªçng '{cache_key}' ch∆∞a c√≥ trong cache, ƒëang t·∫£i t·ª´ disk...")
            load_start = time.time()
            self._prompt_speech_cache[cache_key] = load_prompt_speech_from_file(prompt_speech_file)
            load_duration = time.time() - load_start
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF]   Voice loaded from disk: {load_duration:.3f}s")
            print(f"[{timestamp}] [PERF]   ƒê√£ t·∫£i gi·ªçng t·ª´ disk: {load_duration:.3f}s")
        else:
            print(f"[{timestamp}] [PERF] Voice '{cache_key}' found in cache (instant)")
            print(f"[{timestamp}] [PERF] Gi·ªçng '{cache_key}' c√≥ trong cache (t·ª©c th·ªùi)")
        
        prompt_speech = self._prompt_speech_cache[cache_key]
        step_duration = time.time() - step_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 2 - Voice loading: {step_duration*1000:.2f}ms")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 2 - T·∫£i gi·ªçng: {step_duration*1000:.2f}ms")
        
        # Step 3: Validate text / B∆∞·ªõc 3: X√°c th·ª±c vƒÉn b·∫£n
        step_start = time.time()
        text = text.strip() if isinstance(text, str) else str(text).strip()
        
        if not text or len(text) == 0:
            raise ValueError(
                f"Text is empty. Cannot generate audio from empty text."
            )
        
        meaningful_text = ''.join(c for c in text if c.isalnum() or c.isspace()).strip()
        
        if len(text) < 10 or len(meaningful_text) < 5:
            raise ValueError(
                f"Text is too short or contains only punctuation (length: {len(text)}, meaningful: {len(meaningful_text)}). "
                f"Minimum length: 10 characters with at least 5 meaningful characters. "
                f"Text: '{text[:50] if text else 'None'}...'"
            )
        step_duration = time.time() - step_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 3 - Text validation: {step_duration*1000:.2f}ms")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 3 - X√°c th·ª±c text: {step_duration*1000:.2f}ms")
        
        # Step 4: Generate audio (MAIN BOTTLENECK) / B∆∞·ªõc 4: T·∫°o audio (ƒêI·ªÇM NGH·∫ºN CH√çNH)
        step_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 4 - Starting audio generation (this is the main step)...")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 4 - B·∫Øt ƒë·∫ßu t·∫°o audio (ƒë√¢y l√† b∆∞·ªõc ch√≠nh)...")
        try:
            # Add detailed timing for each step inside tts_to_wav
            # Th√™m timing chi ti·∫øt cho t·ª´ng b∆∞·ªõc b√™n trong tts_to_wav
            wav = self._synthesize_with_detailed_timing(text, prompt_speech, speed)
        except ValueError as e:
            if "need at least one array" in str(e).lower() or "concatenate" in str(e).lower():
                raise ValueError(
                    f"Text processing resulted in empty chunks. "
                    f"Text length: {len(text)} chars. "
                    f"Text preview: {text[:100]}... "
                    f"Original error: {e}"
                )
            raise
        
        step_duration = time.time() - step_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 4 - Audio generation: {step_duration:.3f}s")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 4 - T·∫°o audio: {step_duration:.3f}s")
        
        # Step 5: Validate output / B∆∞·ªõc 5: X√°c th·ª±c ƒë·∫ßu ra
        step_start = time.time()
        if wav is None or len(wav) == 0:
            raise ValueError(
                f"Generated audio is empty. "
                f"Text length: {len(text)} chars. "
                f"Text preview: {text[:100]}..."
            )
        step_duration = time.time() - step_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF] Step 5 - Output validation: {step_duration*1000:.2f}ms")
        print(f"[{timestamp}] [PERF] B∆∞·ªõc 5 - X√°c th·ª±c ƒë·∫ßu ra: {step_duration*1000:.2f}ms")
        
        # Step 6: Save if needed / B∆∞·ªõc 6: L∆∞u n·∫øu c·∫ßn
        step_start = time.time()
        if output_path:
            sf.write(output_path, wav, self.sample_rate)
        step_duration = time.time() - step_start
        if output_path:
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF] Step 6 - Save to file: {step_duration*1000:.2f}ms")
            print(f"[{timestamp}] [PERF] B∆∞·ªõc 6 - L∆∞u file: {step_duration*1000:.2f}ms")
        
        # Total duration
        total_duration = time.time() - total_start
        audio_duration = len(wav) / self.sample_rate if wav is not None else 0
        ratio = total_duration / audio_duration if audio_duration > 0 else 0
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        
        print(f"{'='*60}")
        print(f"[{timestamp}] [PERF] SUMMARY / T·ªîNG K·∫æT:")
        print(f"[{timestamp}] [PERF]   Total time: {total_duration:.3f}s")
        print(f"[{timestamp}] [PERF]   T·ªïng th·ªùi gian: {total_duration:.3f}s")
        print(f"[{timestamp}] [PERF]   Audio duration: {audio_duration:.3f}s")
        print(f"[{timestamp}] [PERF]   ƒê·ªô d√†i audio: {audio_duration:.3f}s")
        print(f"[{timestamp}] [PERF]   Speed ratio: {ratio:.2f}x ({'‚úÖ Real-time' if ratio <= 1.2 else '‚ö†Ô∏è Slower' if ratio <= 2.0 else '‚ùå Too slow'})")
        print(f"[{timestamp}] [PERF]   T·ª∑ l·ªá t·ªëc ƒë·ªô: {ratio:.2f}x ({'‚úÖ Real-time' if ratio <= 1.2 else '‚ö†Ô∏è Ch·∫≠m h∆°n' if ratio <= 2.0 else '‚ùå Qu√° ch·∫≠m'})")
        print(f"{'='*60}\n")
        
        return wav
    
    def _synthesize_with_detailed_timing(self, text: str, prompt_speech, speed: float) -> np.ndarray:
        """
        Synthesize with detailed timing logs to identify bottlenecks.
        T·ªïng h·ª£p v·ªõi timing logs chi ti·∫øt ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm ngh·∫Ωn.
        """
        total_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] Starting detailed synthesis timing...")
        print(f"[{timestamp}] [PERF-DETAIL] B·∫Øt ƒë·∫ßu timing chi ti·∫øt synthesis...")
        
        wavs = []
        chunk_count = 0
        
        # Step 4.1: Text preprocessing / B∆∞·ªõc 4.1: X·ª≠ l√Ω text tr∆∞·ªõc
        preprocess_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] Step 4.1 - Starting text preprocessing...")
        print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.1 - B·∫Øt ƒë·∫ßu x·ª≠ l√Ω text tr∆∞·ªõc...")
        preprocessed_chunks = list(self.model.frontend.preprocess_text(text, split=True))
        preprocess_duration = time.time() - preprocess_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] Step 4.1 - Text preprocessing completed: {preprocess_duration:.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.1 - X·ª≠ l√Ω text tr∆∞·ªõc ho√†n t·∫•t: {preprocess_duration:.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   Number of chunks: {len(preprocessed_chunks)}")
        print(f"[{timestamp}] [PERF-DETAIL]   S·ªë l∆∞·ª£ng chunks: {len(preprocessed_chunks)}")
        
        # Step 4.2: Process each chunk / B∆∞·ªõc 4.2: X·ª≠ l√Ω t·ª´ng chunk
        total_frontend_time = 0
        total_model_time = 0
        
        for chunk_idx, chunk_text in enumerate(preprocessed_chunks):
            chunk_start = time.time()
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL] Step 4.2.{chunk_idx + 1} - Processing chunk {chunk_idx + 1}/{len(preprocessed_chunks)}...")
            print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.2.{chunk_idx + 1} - ƒêang x·ª≠ l√Ω chunk {chunk_idx + 1}/{len(preprocessed_chunks)}...")
            print(f"[{timestamp}] [PERF-DETAIL]   Chunk text length: {len(chunk_text)} chars")
            print(f"[{timestamp}] [PERF-DETAIL]   ƒê·ªô d√†i text chunk: {len(chunk_text)} k√Ω t·ª±")
            
            # Step 4.2.1: Frontend processing / B∆∞·ªõc 4.2.1: X·ª≠ l√Ω frontend
            frontend_start = time.time()
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL]   Step 4.2.{chunk_idx + 1}.1 - Frontend processing (ONNX - may be CPU)...")
            print(f"[{timestamp}] [PERF-DETAIL]   B∆∞·ªõc 4.2.{chunk_idx + 1}.1 - X·ª≠ l√Ω frontend (ONNX - c√≥ th·ªÉ l√† CPU)...")
            model_input = self.model.frontend.frontend_tts(chunk_text, prompt_speech)
            frontend_duration = time.time() - frontend_start
            total_frontend_time += frontend_duration
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL]   Step 4.2.{chunk_idx + 1}.1 - Frontend completed: {frontend_duration:.3f}s")
            print(f"[{timestamp}] [PERF-DETAIL]   B∆∞·ªõc 4.2.{chunk_idx + 1}.1 - Frontend ho√†n t·∫•t: {frontend_duration:.3f}s")
            
            # Step 4.2.2: Model inference / B∆∞·ªõc 4.2.2: Inference model
            model_start = time.time()
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL]   Step 4.2.{chunk_idx + 1}.2 - Model inference (PyTorch GPU)...")
            print(f"[{timestamp}] [PERF-DETAIL]   B∆∞·ªõc 4.2.{chunk_idx + 1}.2 - Inference model (PyTorch GPU)...")
            for model_output in self.model.model.tts(**model_input, stream=False, speed=speed):
                wavs.append(model_output['tts_speech'].squeeze(0).numpy())
                chunk_count += 1
            model_duration = time.time() - model_start
            total_model_time += model_duration
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL]   Step 4.2.{chunk_idx + 1}.2 - Model inference completed: {model_duration:.3f}s")
            print(f"[{timestamp}] [PERF-DETAIL]   B∆∞·ªõc 4.2.{chunk_idx + 1}.2 - Inference model ho√†n t·∫•t: {model_duration:.3f}s")
            
            chunk_duration = time.time() - chunk_start
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            print(f"[{timestamp}] [PERF-DETAIL] Step 4.2.{chunk_idx + 1} - Chunk {chunk_idx + 1} total: {chunk_duration:.3f}s")
            print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.2.{chunk_idx + 1} - Chunk {chunk_idx + 1} t·ªïng: {chunk_duration:.3f}s")
        
        # Step 4.3: Concatenate audio chunks / B∆∞·ªõc 4.3: N·ªëi c√°c chunks audio
        concat_start = time.time()
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] Step 4.3 - Concatenating {len(wavs)} audio chunks...")
        print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.3 - ƒêang n·ªëi {len(wavs)} chunks audio...")
        wav = np.concatenate(wavs, axis=0)
        concat_duration = time.time() - concat_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] Step 4.3 - Concatenation completed: {concat_duration*1000:.2f}ms")
        print(f"[{timestamp}] [PERF-DETAIL] B∆∞·ªõc 4.3 - N·ªëi ho√†n t·∫•t: {concat_duration*1000:.2f}ms")
        
        # Summary
        total_duration = time.time() - total_start
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        print(f"[{timestamp}] [PERF-DETAIL] DETAILED TIMING SUMMARY:")
        print(f"[{timestamp}] [PERF-DETAIL] T·ªîNG K·∫æT TIMING CHI TI·∫æT:")
        print(f"[{timestamp}] [PERF-DETAIL]   Text preprocessing: {preprocess_duration:.3f}s ({preprocess_duration/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   X·ª≠ l√Ω text tr∆∞·ªõc: {preprocess_duration:.3f}s ({preprocess_duration/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   Frontend processing (ONNX): {total_frontend_time:.3f}s ({total_frontend_time/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   X·ª≠ l√Ω frontend (ONNX): {total_frontend_time:.3f}s ({total_frontend_time/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   Model inference (PyTorch GPU): {total_model_time:.3f}s ({total_model_time/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   Inference model (PyTorch GPU): {total_model_time:.3f}s ({total_model_time/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   Audio concatenation: {concat_duration:.3f}s ({concat_duration/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   N·ªëi audio: {concat_duration:.3f}s ({concat_duration/total_duration*100:.1f}%)")
        print(f"[{timestamp}] [PERF-DETAIL]   Total: {total_duration:.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   T·ªïng: {total_duration:.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   Number of chunks: {len(preprocessed_chunks)}")
        print(f"[{timestamp}] [PERF-DETAIL]   S·ªë l∆∞·ª£ng chunks: {len(preprocessed_chunks)}")
        print(f"[{timestamp}] [PERF-DETAIL]   Average frontend time per chunk: {total_frontend_time/len(preprocessed_chunks):.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   Th·ªùi gian frontend trung b√¨nh m·ªói chunk: {total_frontend_time/len(preprocessed_chunks):.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   Average model time per chunk: {total_model_time/len(preprocessed_chunks):.3f}s")
        print(f"[{timestamp}] [PERF-DETAIL]   Th·ªùi gian model trung b√¨nh m·ªói chunk: {total_model_time/len(preprocessed_chunks):.3f}s")
        
        return wav
    
    def get_sample_rate(self) -> int:
        """Get sample rate / L·∫•y t·∫ßn s·ªë l·∫•y m·∫´u"""
        return self.sample_rate
    
    def list_voices(self) -> dict:
        """
        List available voices / Li·ªát k√™ c√°c gi·ªçng c√≥ s·∫µn
        
        Returns:
            Dictionary mapping voice names to file paths / T·ª´ ƒëi·ªÉn √°nh x·∫° t√™n gi·ªçng ƒë·∫øn ƒë∆∞·ªùng d·∫´n file
        """
        return self.voice_map.copy()

