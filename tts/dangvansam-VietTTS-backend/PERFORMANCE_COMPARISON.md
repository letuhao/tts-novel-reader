# Performance Comparison: Original vs Current Setup
# So sÃ¡nh Hiá»‡u suáº¥t: Setup Gá»‘c vs Setup Hiá»‡n táº¡i

## Problem / Váº¥n Ä‘á»

**Original setup** (same as `viet-tts`): **Near real-time** performance âš¡  
**Current setup** (after optimizations): **Very slow** ğŸŒ

**Setup gá»‘c** (giá»‘ng `viet-tts`): Hiá»‡u suáº¥t **gáº§n real-time** âš¡  
**Setup hiá»‡n táº¡i** (sau tá»‘i Æ°u hÃ³a): **Ráº¥t cháº­m** ğŸŒ

## Original Setup (Fast) / Setup Gá»‘c (Nhanh)

**File:** `tts/viet-tts/viettts/server.py`

### Simple Initialization / Khá»Ÿi táº¡o ÄÆ¡n giáº£n
```python
@app.on_event("startup")
async def startup():
    global tts_obj
    tts_obj = TTS('./pretrained-models')  # Direct, simple
```

### Direct Inference / Inference Trá»±c tiáº¿p
```python
model_output = tts_obj.inference_tts(
    tts_text=text,
    prompt_speech_16k=prompt_speech_16k,
    speed=speed,
    stream=False
)
```

### Characteristics / Äáº·c Ä‘iá»ƒm:
- âœ… **Direct model instantiation** / Khá»Ÿi táº¡o model trá»±c tiáº¿p
- âœ… **No wrapper layers** / KhÃ´ng cÃ³ lá»›p wrapper
- âœ… **No warmup** / KhÃ´ng warmup
- âœ… **No executor** / KhÃ´ng executor
- âœ… **No pool** / KhÃ´ng pool
- âœ… **Minimal overhead** / Overhead tá»‘i thiá»ƒu
- âœ… **Near real-time** / Gáº§n real-time

## Current Setup (Slow) / Setup Hiá»‡n táº¡i (Cháº­m)

### Multiple Layers / Nhiá»u Lá»›p:

```
Request â†’ API (async/executor) â†’ Service (pool) â†’ Model Pool (queue) â†’ Wrapper â†’ TTS â†’ Model
```

### Performance Overhead / Overhead Hiá»‡u suáº¥t:

#### 1. Model Pool Initialization / Khá»Ÿi táº¡o Model Pool
- **Time:** 60-120 seconds (2 instances Ã— 30-60s warmup each)
- **Thá»i gian:** 60-120 giÃ¢y (2 instances Ã— 30-60s warmup má»—i instance)
- **Impact:** Slow startup / Khá»Ÿi Ä‘á»™ng cháº­m

#### 2. Warmup Function / HÃ m Warmup
- **Time:** 30-60 seconds per instance
- **Thá»i gian:** 30-60 giÃ¢y má»—i instance
- **Impact:** Adds startup delay / ThÃªm Ä‘á»™ trá»… khá»Ÿi Ä‘á»™ng

#### 3. Executor Wrapper / Wrapper Executor
```python
audio = await loop.run_in_executor(None, lambda: service.synthesize(...))
```
- **Impact:** Thread pool overhead, context switching / Overhead thread pool, chuyá»ƒn context

#### 4. Model Pool Queue / HÃ ng Ä‘á»£i Model Pool
```python
with self.model_pool.get_model() as viet_tts:
    return viet_tts.synthesize(...)
```
- **Impact:** Queue get/put overhead, lock contention / Overhead get/put queue, xung Ä‘á»™t lock

#### 5. Multiple Wrapper Layers / Nhiá»u Lá»›p Wrapper
- API layer â†’ Service layer â†’ Pool â†’ Wrapper â†’ TTS â†’ Model
- **Impact:** Function call overhead / Overhead gá»i hÃ m

#### 6. Text Validation / XÃ¡c thá»±c VÄƒn báº£n
```python
meaningful_text = ''.join(c for c in text if c.isalnum() or c.isspace()).strip()
```
- **Impact:** String processing overhead / Overhead xá»­ lÃ½ string

#### 7. Audio Conversion / Chuyá»ƒn Ä‘á»•i Audio
```python
audio_buffer = io.BytesIO()
sf.write(audio_buffer, audio, sample_rate, format="WAV")
```
- **Impact:** Format conversion overhead / Overhead chuyá»ƒn Ä‘á»•i format

## What Made It Fast / Äiá»u GÃ¬ LÃ m NÃ³ Nhanh

1. **Direct Calls / Gá»i Trá»±c tiáº¿p:**
   ```python
   tts_obj = TTS(model_dir)  # Simple
   output = tts_obj.inference_tts(...)  # Direct
   ```

2. **No Warmup / KhÃ´ng Warmup:**
   - First inference compiles CUDA kernels automatically
   - Subsequent inferences are fast
   - Inference Ä‘áº§u tiÃªn tá»± Ä‘á»™ng compile CUDA kernels
   - CÃ¡c inference tiáº¿p theo nhanh

3. **No Pool / KhÃ´ng Pool:**
   - Single instance, direct access
   - No queue overhead
   - Má»™t instance, truy cáº­p trá»±c tiáº¿p
   - KhÃ´ng cÃ³ overhead queue

4. **No Executor / KhÃ´ng Executor:**
   - Direct function calls
   - No thread switching
   - Gá»i hÃ m trá»±c tiáº¿p
   - KhÃ´ng chuyá»ƒn thread

5. **Minimal Validation / XÃ¡c thá»±c Tá»‘i thiá»ƒu:**
   - Let model handle edge cases
   - No extra processing
   - Äá»ƒ model xá»­ lÃ½ edge cases
   - KhÃ´ng xá»­ lÃ½ thÃªm

## Solution: Simplify to Original Pattern / Giáº£i phÃ¡p: ÄÆ¡n giáº£n hÃ³a vá» Pattern Gá»‘c

### Remove These / Loáº¡i bá» CÃ¡c Thá»© NÃ y:

1. âœ… **Model Pool** (use single instance)
2. âœ… **Warmup** (let first inference compile kernels)
3. âœ… **Executor wrapper** (direct calls)
4. âœ… **Extra validation** (minimal checks only)

### Keep These / Giá»¯ láº¡i CÃ¡c Thá»© NÃ y:

- âœ… Basic wrapper for API integration
- âœ… Error handling
- âœ… Device detection
- âœ… Voice selection logic

## Expected Performance / Hiá»‡u suáº¥t Dá»± kiáº¿n

After simplifying:
- **Startup:** 60-120s â†’ 5-10s
- **First inference:** Same (CUDA kernels compile automatically)
- **Subsequent inference:** Near real-time (like original)

Sau khi Ä‘Æ¡n giáº£n hÃ³a:
- **Khá»Ÿi Ä‘á»™ng:** 60-120s â†’ 5-10s
- **Inference Ä‘áº§u tiÃªn:** Giá»‘ng nhau (CUDA kernels tá»± Ä‘á»™ng compile)
- **Inference tiáº¿p theo:** Gáº§n real-time (nhÆ° gá»‘c)
