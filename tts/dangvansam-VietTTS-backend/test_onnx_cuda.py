"""
Test ONNX Runtime CUDA provider to identify the exact error
Kiá»ƒm tra ONNX Runtime CUDA provider Ä‘á»ƒ xÃ¡c Ä‘á»‹nh lá»—i chÃ­nh xÃ¡c
"""
import onnxruntime
import numpy as np
import torch

print("ğŸ” Testing ONNX Runtime CUDA Provider...")
print("ğŸ” Äang kiá»ƒm tra ONNX Runtime CUDA Provider...")
print()

# Check providers
print("Available providers:", onnxruntime.get_available_providers())
print()

# Try to create a dummy session with CUDA
# We need a real ONNX model file, but let's see what error we get
print("Testing CUDA provider initialization...")
print("Äang kiá»ƒm tra khá»Ÿi táº¡o CUDA provider...")

# Check if we can at least import the CUDA provider
try:
    # Try to get provider info
    providers = onnxruntime.get_available_providers()
    print(f"âœ… Providers available: {providers}")
    
    if "CUDAExecutionProvider" in providers:
        print("âœ… CUDAExecutionProvider is in available providers")
        
        # Try to create a session (this will fail without a model, but we'll see the error)
        sess_opts = onnxruntime.SessionOptions()
        print("âœ… SessionOptions created")
        
        # The actual error happens when creating InferenceSession with a model
        print("âš ï¸  To fully test, we need an actual ONNX model file")
        print("âš ï¸  Äá»ƒ kiá»ƒm tra Ä‘áº§y Ä‘á»§, chÃºng ta cáº§n file ONNX model thá»±c táº¿")
        
except Exception as e:
    print(f"âŒ Error: {e}")
    print(f"âŒ Lá»—i: {e}")
    import traceback
    traceback.print_exc()

print()
print("ğŸ’¡ The WinError 193 happens when ONNX Runtime tries to load CUDA DLLs")
print("ğŸ’¡ Lá»—i WinError 193 xáº£y ra khi ONNX Runtime cá»‘ gáº¯ng táº£i DLL CUDA")
print("   This is usually a dependency issue (missing CUDA DLLs or wrong architecture)")
print("   ÄÃ¢y thÆ°á»ng lÃ  váº¥n Ä‘á» phá»¥ thuá»™c (thiáº¿u DLL CUDA hoáº·c kiáº¿n trÃºc sai)")

